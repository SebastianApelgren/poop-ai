{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79638fe9",
   "metadata": {},
   "source": [
    "# Advanced Training Pipeline\n",
    "\n",
    "This notebook implements:\n",
    "1. Stronger and more varied augmentation, including class-specific oversampling.\n",
    "2. Model-level adjustments: gradual unfreezing, EfficientNet-B0/B3, label smoothing, focal loss/class-weighted loss.\n",
    "3. Training strategies: early stopping, checkpoint ensembles, and k-fold cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bc1ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a4ae308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = \"../data\"  # Update this path\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16 # Adjust based on your GPU memory\n",
    "NUM_CLASSES = 7\n",
    "LR = 1e-4\n",
    "NUM_EPOCHS = 20\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "KFOLDS = 5\n",
    "\n",
    "# Freeze strategy: 'features.4', 'features.5', 'features.6', etc.\n",
    "FREEZE_UNTIL_BLOCK = 'features.3'\n",
    "\n",
    "# Loss function: 'focal' or 'smooth'\n",
    "LOSS_TYPE = 'focal'\n",
    "\n",
    "# Seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd323500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoolDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.class_to_idx = {}\n",
    "        for idx, class_name in enumerate(sorted(os.listdir(root_dir))):\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                self.class_to_idx[class_name] = idx\n",
    "                for fname in os.listdir(class_path):\n",
    "                    if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        self.samples.append((os.path.join(class_path, fname), idx))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "481bd449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stronger and more varied augmentations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),  # random crop + resize\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(3)], p=0.2),\n",
    "    transforms.RandomApply([transforms.Lambda(lambda img: img.filter(ImageFilter.FIND_EDGES))], p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "340ab4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Smoothing Loss (CrossEntropy with label_smoothing)\n",
    "criterion_smooth = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Focal Loss Implementation\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * ((1 - pt) ** self.gamma) * ce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f3691ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(backbone='efficientnet_b0', num_classes=NUM_CLASSES, freeze_until_layer=None):\n",
    "    # Load pretrained EfficientNet\n",
    "    if backbone == 'efficientnet_b0':\n",
    "        model = models.efficientnet_b0(pretrained=True)\n",
    "    elif backbone == 'efficientnet_b3':\n",
    "        model = models.efficientnet_b3(pretrained=True)\n",
    "    else:\n",
    "        raise ValueError('Invalid backbone')\n",
    "    \n",
    "    # Replace classifier head\n",
    "    in_features = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(in_features, 512),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    \n",
    "    # Freeze layers if specified\n",
    "    if freeze_until_layer:\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "            if name.startswith(freeze_until_layer):\n",
    "                break\n",
    "        # Unfreeze subsequent layers\n",
    "        unfreeze = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if unfreeze:\n",
    "                param.requires_grad = True\n",
    "            if name.startswith(freeze_until_layer):\n",
    "                unfreeze = True\n",
    "    \n",
    "    return model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb02600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return None, None, all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84c606df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate(model, train_loader, val_loader, criterion, optimizer, num_epochs, fold_idx):\n",
    "    best_acc = 0.0\n",
    "    best_weights = None\n",
    "    patience = 3\n",
    "    counter = 0\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = running_corrects / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        val_total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_corrects += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_loss_epoch = val_loss / val_total\n",
    "        val_acc_epoch = val_corrects / val_total\n",
    "        lr_scheduler.step(val_acc_epoch)\n",
    "\n",
    "        print(f\"Fold {fold_idx}, Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f} - \"\n",
    "              f\"Val Loss: {val_loss_epoch:.4f}, Val Acc: {val_acc_epoch:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_acc_epoch > best_acc:\n",
    "            best_acc = val_acc_epoch\n",
    "            best_weights = model.state_dict().copy()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # Load best weights\n",
    "    model.load_state_dict(best_weights)\n",
    "\n",
    "    # Final validation metrics\n",
    "    _, _, preds, labels = evaluate_model(model, val_loader)\n",
    "    print(\"\\nClassification Report for Fold {}:\".format(fold_idx))\n",
    "    print(classification_report(labels, preds, target_names=sorted(os.listdir(DATA_DIR))))\n",
    "\n",
    "    return model, best_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a45e2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find ../data -name \".DS_Store\" -type f -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d1cc5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in full dataset: 1006\n",
      "They are these classes: ['type-1', 'type-2', 'type-3', 'type-4', 'type-5', 'type-6', 'type-7']\n",
      "class_counts: [122 126 326 304  28  25  75]\n",
      "class_weights: [0.00819672 0.00793651 0.00306748 0.00328947 0.03571429 0.04\n",
      " 0.01333333]\n"
     ]
    }
   ],
   "source": [
    "# Prepare full dataset indices for k-fold\n",
    "full_dataset = StoolDataset(DATA_DIR, transform=None)\n",
    "indices = list(range(len(full_dataset)))\n",
    "\n",
    "print(f\"Total samples in full dataset: {len(full_dataset)}\")\n",
    "print(f\"They are these classes: {sorted(os.listdir(DATA_DIR))}\")\n",
    "\n",
    "# Calculate class weights for full dataset\n",
    "all_labels_full = [label for _, label in full_dataset]\n",
    "class_counts = np.bincount(all_labels_full)\n",
    "class_weights = 1.0 / class_counts\n",
    "\n",
    "\n",
    "print('class_counts:', class_counts)\n",
    "print('class_weights:', class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d730e0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Fold 1 =======\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b3_rwightman-b3899882.pth\" to /Users/sebastianapelgren/.cache/torch/hub/checkpoints/efficientnet_b3_rwightman-b3899882.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 47.2M/47.2M [00:00<00:00, 70.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1/20 - Train Loss: 1.3135, Train Acc: 0.3085 - Val Loss: 1.1509, Val Acc: 0.3960\n",
      "Fold 1, Epoch 2/20 - Train Loss: 0.9320, Train Acc: 0.5025 - Val Loss: 0.7318, Val Acc: 0.5099\n",
      "Fold 1, Epoch 3/20 - Train Loss: 0.5344, Train Acc: 0.6604 - Val Loss: 0.5696, Val Acc: 0.5248\n",
      "Fold 1, Epoch 4/20 - Train Loss: 0.4653, Train Acc: 0.6704 - Val Loss: 0.4294, Val Acc: 0.6139\n",
      "Fold 1, Epoch 5/20 - Train Loss: 0.4191, Train Acc: 0.7139 - Val Loss: 0.3900, Val Acc: 0.6139\n",
      "Fold 1, Epoch 6/20 - Train Loss: 0.3656, Train Acc: 0.7289 - Val Loss: 0.3904, Val Acc: 0.6535\n",
      "Fold 1, Epoch 7/20 - Train Loss: 0.3487, Train Acc: 0.7326 - Val Loss: 0.4454, Val Acc: 0.6040\n",
      "Fold 1, Epoch 8/20 - Train Loss: 0.3155, Train Acc: 0.7550 - Val Loss: 0.3866, Val Acc: 0.6535\n",
      "Fold 1, Epoch 9/20 - Train Loss: 0.2821, Train Acc: 0.7960 - Val Loss: 0.3385, Val Acc: 0.6733\n",
      "Fold 1, Epoch 10/20 - Train Loss: 0.2689, Train Acc: 0.7898 - Val Loss: 0.3435, Val Acc: 0.6238\n",
      "Fold 1, Epoch 11/20 - Train Loss: 0.2377, Train Acc: 0.8172 - Val Loss: 0.3155, Val Acc: 0.6634\n",
      "Fold 1, Epoch 12/20 - Train Loss: 0.1973, Train Acc: 0.8557 - Val Loss: 0.3555, Val Acc: 0.6881\n",
      "Fold 1, Epoch 13/20 - Train Loss: 0.2160, Train Acc: 0.8396 - Val Loss: 0.2935, Val Acc: 0.7228\n",
      "Fold 1, Epoch 14/20 - Train Loss: 0.2120, Train Acc: 0.8308 - Val Loss: 0.2994, Val Acc: 0.7228\n",
      "Fold 1, Epoch 15/20 - Train Loss: 0.2044, Train Acc: 0.8507 - Val Loss: 0.2938, Val Acc: 0.7178\n",
      "Fold 1, Epoch 16/20 - Train Loss: 0.1744, Train Acc: 0.8557 - Val Loss: 0.3404, Val Acc: 0.6881\n",
      "Early stopping at epoch 16\n",
      "\n",
      "Classification Report for Fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      type-1       0.75      0.88      0.81        24\n",
      "      type-2       0.64      0.90      0.75        20\n",
      "      type-3       0.76      0.46      0.57        76\n",
      "      type-4       0.59      0.79      0.68        57\n",
      "      type-5       0.62      0.71      0.67         7\n",
      "      type-6       1.00      0.67      0.80         6\n",
      "      type-7       0.92      0.92      0.92        12\n",
      "\n",
      "    accuracy                           0.69       202\n",
      "   macro avg       0.76      0.76      0.74       202\n",
      "weighted avg       0.71      0.69      0.68       202\n",
      "\n",
      "======= Fold 2 =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 1/20 - Train Loss: 1.3220, Train Acc: 0.3006 - Val Loss: 1.1902, Val Acc: 0.3433\n",
      "Fold 2, Epoch 2/20 - Train Loss: 0.9311, Train Acc: 0.5280 - Val Loss: 0.7945, Val Acc: 0.4975\n",
      "Fold 2, Epoch 3/20 - Train Loss: 0.6303, Train Acc: 0.6124 - Val Loss: 0.6596, Val Acc: 0.4428\n"
     ]
    }
   ],
   "source": [
    "# Prepare full dataset indices for k-fold\n",
    "full_dataset = StoolDataset(DATA_DIR, transform=None)\n",
    "indices = list(range(len(full_dataset)))\n",
    "\n",
    "# Calculate class weights for full dataset\n",
    "all_labels_full = [label for _, label in full_dataset]\n",
    "class_counts = np.bincount(all_labels_full)\n",
    "class_weights = 1.0 / class_counts\n",
    "weights_full = [class_weights[label] for label in all_labels_full]\n",
    "\n",
    "kf = KFold(n_splits=KFOLDS, shuffle=True, random_state=SEED)\n",
    "fold_models = []\n",
    "fold_accuracies = []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kf.split(indices), 1):\n",
    "    print(f\"======= Fold {fold_idx} =======\")\n",
    "    # Subset transforms\n",
    "    train_ds = torch.utils.data.Subset(StoolDataset(DATA_DIR, transform=train_transforms), train_idx)\n",
    "    val_ds = torch.utils.data.Subset(StoolDataset(DATA_DIR, transform=val_transforms), val_idx)\n",
    "\n",
    "    # Create weighted sampler for train_ds\n",
    "    train_labels_fold = [train_ds.dataset.samples[i][1] for i in train_idx]\n",
    "    class_sample_count_fold = np.array([train_labels_fold.count(i) for i in range(NUM_CLASSES)])\n",
    "    class_weights_fold = 1.0 / class_sample_count_fold\n",
    "    sample_weights_fold = np.array([class_weights_fold[label] for label in train_labels_fold])\n",
    "    sample_weights_fold = torch.from_numpy(sample_weights_fold.astype(np.double))\n",
    "    sampler_fold = WeightedRandomSampler(sample_weights_fold, num_samples=len(sample_weights_fold), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler_fold)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Create model and freeze initial layers\n",
    "    # model = create_model(backbone='efficientnet_b0')\n",
    "    model = create_model(backbone='efficientnet_b3', freeze_until_layer=FREEZE_UNTIL_BLOCK)\n",
    "    # Optionally freeze until a certain layer name, e.g., 'features.4'\n",
    "    # model = create_model(backbone='efficientnet_b0', freeze_until_layer='features.4')\n",
    "\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n",
    "    # Choose loss: label smoothing or focal\n",
    "    # criterion = criterion_smooth\n",
    "    criterion = FocalLoss(alpha=1, gamma=2)\n",
    "\n",
    "    # Train and validate\n",
    "    best_model, best_acc = train_validate(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, fold_idx)\n",
    "    fold_models.append(best_model)\n",
    "    fold_accuracies.append(best_acc)\n",
    "\n",
    "# Summary of fold accuracies\n",
    "print(\"\\nFold Accuracies:\", fold_accuracies)\n",
    "print(\"Mean Accuracy:\", np.mean(fold_accuracies))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda2c28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of fold accuracies\n",
    "print(\"\\nFold Accuracies:\", fold_accuracies)\n",
    "print(\"Mean Accuracy:\", np.mean(fold_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe743fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Ensemble inference on a test image\n",
    "def ensemble_predict(models, image_path, transform):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    img_t = transform(image).unsqueeze(0).to(DEVICE)\n",
    "    probs = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(img_t)\n",
    "            probs.append(F.softmax(outputs, dim=1).cpu().numpy())\n",
    "    avg_probs = np.mean(np.vstack(probs), axis=0)\n",
    "    pred_class = np.argmax(avg_probs)\n",
    "    return sorted(os.listdir(DATA_DIR))[pred_class], np.max(avg_probs)\n",
    "\n",
    "# Usage example (replace 'some_image.jpg')\n",
    "# label, confidence = ensemble_predict(fold_models, 'some_image.jpg', val_transforms)\n",
    "# print(f\"Ensembled Prediction: {label}, Confidence: {confidence:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dc010c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to stool_model.pth\n"
     ]
    }
   ],
   "source": [
    "# 1. Define where to save\n",
    "SAVE_PATH = \"../api/stool_model.pth\"\n",
    "\n",
    "# 2. Save the state_dict\n",
    "#torch.save(model.state_dict(), SAVE_PATH)\n",
    "print(f\"Model weights saved to {SAVE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
