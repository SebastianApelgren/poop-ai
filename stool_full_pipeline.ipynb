{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f99f9731",
   "metadata": {},
   "source": [
    "# Full Training, Evaluation, and Deployment Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26dd0782",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c611233f",
   "metadata": {},
   "source": [
    "# ====== Configuration ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad846d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Configuration ======\n",
    "DATA_DIR = \"path_to_your_unzipped_dataset/data\"  # <-- update this path\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 7\n",
    "LR = 1e-4\n",
    "EPOCHS = 10\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "MODEL_SAVE_PATH = \"stool_resnet18.pth\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b622423",
   "metadata": {},
   "source": [
    "# ====== Transforms ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0323ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59805f4d",
   "metadata": {},
   "source": [
    "# ====== Dataset Definition ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ad26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StoolDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.class_to_idx = {}\n",
    "        for idx, class_name in enumerate(sorted(os.listdir(root_dir))):\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                self.class_to_idx[class_name] = idx\n",
    "                for fname in os.listdir(class_path):\n",
    "                    self.samples.append((os.path.join(class_path, fname), idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e295cc5a",
   "metadata": {},
   "source": [
    "# ====== Load and Split Dataset ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = StoolDataset(DATA_DIR, transform=train_transforms)\n",
    "val_size = int(0.2 * len(full_dataset))\n",
    "train_size = len(full_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad4277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update transform for validation set\n",
    "val_dataset.dataset.transform = val_transforms\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d68d557",
   "metadata": {},
   "source": [
    "# ====== Model Definition ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461073a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d811af8",
   "metadata": {},
   "source": [
    "# ====== Training & Validation Loop ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88b5e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = correct / total * 100\n",
    "    return epoch_loss, epoch_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = correct / total * 100\n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086a3fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accuracies = [], []\n",
    "val_losses, val_accuracies = [], []\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, DEVICE)\n",
    "    train_losses.append(train_loss); train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss); val_accuracies.append(val_acc)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee9c0ef",
   "metadata": {},
   "source": [
    "# ====== Compute Confusion Matrix ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce31a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, preds, labels = evaluate(model, val_loader, criterion, DEVICE)\n",
    "cm = confusion_matrix(labels, preds)\n",
    "class_names = sorted(os.listdir(DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e1c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix - Validation Set\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names, rotation=45)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1593be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(labels, preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1797ee6",
   "metadata": {},
   "source": [
    "# ====== Save Model ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7951c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"Model saved to {MODEL_SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea74a3f",
   "metadata": {},
   "source": [
    "# ====== Example FastAPI Deployment ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2509a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this part as 'app.py' to deploy with `uvicorn app:app --reload`\n",
    "fastapi_code = '''\n",
    "from fastapi import FastAPI, File, UploadFile\n",
    "from PIL import Image\n",
    "import io\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 7\n",
    "MODEL_PATH = \"stool_resnet18.pth\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class_labels = sorted(os.listdir(\"path_to_your_unzipped_dataset/data\"))\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(image_file: UploadFile = File(...)):\n",
    "    contents = await image_file.read()\n",
    "    image = Image.open(io.BytesIO(contents)).convert(\"RGB\")\n",
    "    img_t = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_t)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        top_prob, top_cls = torch.max(probs, 1)\n",
    "        predicted_label = class_labels[top_cls.item()]\n",
    "        confidence = top_prob.item()\n",
    "    return {\"predicted_type\": predicted_label, \"confidence\": confidence}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write FastAPI code to file\n",
    "with open(\"app.py\", \"w\") as f:\n",
    "    f.write(fastapi_code)\n",
    "print(\"FastAPI example code saved to 'app.py'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
