{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ac3341",
   "metadata": {},
   "source": [
    "# Full Training Pipeline with Augmentation and Class Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e96d12dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11337937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def remove_ds_store_files(root_dir):\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        for f in files:\n",
    "            if f == '.DS_Store':\n",
    "                full_path = os.path.join(subdir, f)\n",
    "                os.remove(full_path)\n",
    "                print(f\"Removed: {full_path}\")\n",
    "\n",
    "# Run this once before DataLoader is created\n",
    "remove_ds_store_files('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eac4fe",
   "metadata": {},
   "source": [
    "# ====== Configuration ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"path_to_your_unzipped_dataset/data\"  # <-- update this path\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 7\n",
    "LR = 1e-4\n",
    "EPOCHS = 10\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "MODEL_SAVE_PATH = \"stool_resnet18_balanced.pth\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8e16e5",
   "metadata": {},
   "source": [
    "# ====== Transforms ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0bfc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236fe5ed",
   "metadata": {},
   "source": [
    "# ====== Dataset Definition ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e299854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoolDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.class_to_idx = {}\n",
    "        for idx, class_name in enumerate(sorted(os.listdir(root_dir))):\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                self.class_to_idx[class_name] = idx\n",
    "                for fname in os.listdir(class_path):\n",
    "                    self.samples.append((os.path.join(class_path, fname), idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693c9f8",
   "metadata": {},
   "source": [
    "# ====== Load and Split Dataset ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad390103",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = StoolDataset(DATA_DIR, transform=train_transforms)\n",
    "val_size = int(0.2 * len(full_dataset))\n",
    "train_size = len(full_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Update transform for validation set\n",
    "val_dataset.dataset.transform = val_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d7aa39",
   "metadata": {},
   "source": [
    "# ====== Compute class weights for sampler ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184f8197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count labels in train_dataset\n",
    "train_labels = [label for _, label in train_dataset]\n",
    "class_sample_count = np.array([train_labels.count(i) for i in range(NUM_CLASSES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight for each class: inverse of count\n",
    "class_weights = 1.0 / class_sample_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75718324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight for each sample\n",
    "sample_weights = np.array([class_weights[label] for label in train_labels])\n",
    "sample_weights = torch.from_numpy(sample_weights.astype(np.double))\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a78881c",
   "metadata": {},
   "source": [
    "# ====== DataLoaders ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc6534",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a40993",
   "metadata": {},
   "source": [
    "# ====== Model Definition ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f2b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792d5a38",
   "metadata": {},
   "source": [
    "# ====== Training & Validation Loop ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee6609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = correct / total * 100\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b794f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = correct / total * 100\n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557314f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_losses, train_accuracies = [], []\n",
    "val_losses, val_accuracies = [], []\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, DEVICE)\n",
    "    train_losses.append(train_loss); train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss); val_accuracies.append(val_acc)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721cd632",
   "metadata": {},
   "source": [
    "# ====== Compute Confusion Matrix ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0409d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, preds, labels = evaluate(model, val_loader, criterion, DEVICE)\n",
    "cm = confusion_matrix(labels, preds)\n",
    "class_names = sorted(os.listdir(DATA_DIR))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix - Validation Set\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names, rotation=45)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(labels, preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51049bf1",
   "metadata": {},
   "source": [
    "# ====== Save Model ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b86cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"Model saved to {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be50cc5",
   "metadata": {},
   "source": [
    "# ====== Example FastAPI Deployment ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd5664",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastapi_code = '''\n",
    "from fastapi import FastAPI, File, UploadFile\n",
    "from PIL import Image\n",
    "import io\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 7\n",
    "MODEL_PATH = \"stool_resnet18_balanced.pth\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class_labels = sorted(os.listdir(\"path_to_your_unzipped_dataset/data\"))\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(image_file: UploadFile = File(...)):\n",
    "    contents = await image_file.read()\n",
    "    image = Image.open(io.BytesIO(contents)).convert(\"RGB\")\n",
    "    img_t = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_t)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        top_prob, top_cls = torch.max(probs, 1)\n",
    "        predicted_label = class_labels[top_cls.item()]\n",
    "        confidence = top_prob.item()\n",
    "    return {\"predicted_type\": predicted_label, \"confidence\": confidence}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b98fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write FastAPI code to file\n",
    "with open(\"app_balanced.py\", \"w\") as f:\n",
    "    f.write(fastapi_code)\n",
    "print(\"FastAPI example code with balanced sampling saved to 'app_balanced.py'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
