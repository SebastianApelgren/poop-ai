{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79638fe9",
   "metadata": {},
   "source": [
    "# Advanced Training Pipeline\n",
    "\n",
    "This notebook implements:\n",
    "1. Stronger and more varied augmentation, including class-specific oversampling.\n",
    "2. Model-level adjustments: gradual unfreezing, EfficientNet-B0/B3, label smoothing, focal loss/class-weighted loss.\n",
    "3. Training strategies: early stopping, checkpoint ensembles, and k-fold cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bc1ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a4ae308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = \"data\"  # Update this path\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 7\n",
    "LR = 1e-4\n",
    "NUM_EPOCHS = 20\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "KFOLDS = 5\n",
    "\n",
    "# Seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd323500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoolDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.class_to_idx = {}\n",
    "        for idx, class_name in enumerate(sorted(os.listdir(root_dir))):\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                self.class_to_idx[class_name] = idx\n",
    "                for fname in os.listdir(class_path):\n",
    "                    if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        self.samples.append((os.path.join(class_path, fname), idx))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "481bd449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stronger and more varied augmentations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),  # random crop + resize\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(3)], p=0.2),\n",
    "    transforms.RandomApply([transforms.Lambda(lambda img: img.filter(ImageFilter.FIND_EDGES))], p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "340ab4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Smoothing Loss (CrossEntropy with label_smoothing)\n",
    "criterion_smooth = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Focal Loss Implementation\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * ((1 - pt) ** self.gamma) * ce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f3691ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(backbone='efficientnet_b0', num_classes=NUM_CLASSES, freeze_until_layer=None):\n",
    "    # Load pretrained EfficientNet\n",
    "    if backbone == 'efficientnet_b0':\n",
    "        model = models.efficientnet_b0(pretrained=True)\n",
    "    elif backbone == 'efficientnet_b3':\n",
    "        model = models.efficientnet_b3(pretrained=True)\n",
    "    else:\n",
    "        raise ValueError('Invalid backbone')\n",
    "    \n",
    "    # Replace classifier head\n",
    "    in_features = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(in_features, 512),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    \n",
    "    # Freeze layers if specified\n",
    "    if freeze_until_layer:\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "            if name.startswith(freeze_until_layer):\n",
    "                break\n",
    "        # Unfreeze subsequent layers\n",
    "        unfreeze = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if unfreeze:\n",
    "                param.requires_grad = True\n",
    "            if name.startswith(freeze_until_layer):\n",
    "                unfreeze = True\n",
    "    \n",
    "    return model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb02600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return None, None, all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84c606df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate(model, train_loader, val_loader, criterion, optimizer, num_epochs, fold_idx):\n",
    "    best_acc = 0.0\n",
    "    best_weights = None\n",
    "    patience = 3\n",
    "    counter = 0\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = running_corrects / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        val_total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_corrects += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_loss_epoch = val_loss / val_total\n",
    "        val_acc_epoch = val_corrects / val_total\n",
    "        lr_scheduler.step(val_acc_epoch)\n",
    "\n",
    "        print(f\"Fold {fold_idx}, Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f} - \"\n",
    "              f\"Val Loss: {val_loss_epoch:.4f}, Val Acc: {val_acc_epoch:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_acc_epoch > best_acc:\n",
    "            best_acc = val_acc_epoch\n",
    "            best_weights = model.state_dict().copy()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # Load best weights\n",
    "    model.load_state_dict(best_weights)\n",
    "\n",
    "    # Final validation metrics\n",
    "    _, _, preds, labels = evaluate_model(model, val_loader)\n",
    "    print(\"\\nClassification Report for Fold {}:\".format(fold_idx))\n",
    "    print(classification_report(labels, preds, target_names=sorted(os.listdir(DATA_DIR))))\n",
    "\n",
    "    return model, best_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d730e0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Fold 1 =======\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /Users/sebastianapelgren/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 20.5M/20.5M [00:00<00:00, 32.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1/20 - Train Loss: 1.4219, Train Acc: 0.2153 - Val Loss: 1.3573, Val Acc: 0.2703\n",
      "Fold 1, Epoch 2/20 - Train Loss: 1.3040, Train Acc: 0.4236 - Val Loss: 1.2128, Val Acc: 0.5405\n",
      "Fold 1, Epoch 3/20 - Train Loss: 1.2324, Train Acc: 0.3819 - Val Loss: 0.9646, Val Acc: 0.6216\n",
      "Fold 1, Epoch 4/20 - Train Loss: 1.0800, Train Acc: 0.5069 - Val Loss: 0.7871, Val Acc: 0.7568\n",
      "Fold 1, Epoch 5/20 - Train Loss: 0.9351, Train Acc: 0.5903 - Val Loss: 0.6440, Val Acc: 0.7027\n",
      "Fold 1, Epoch 6/20 - Train Loss: 0.7867, Train Acc: 0.6042 - Val Loss: 0.4696, Val Acc: 0.7297\n",
      "Fold 1, Epoch 7/20 - Train Loss: 0.6370, Train Acc: 0.6528 - Val Loss: 0.3705, Val Acc: 0.7568\n",
      "Early stopping at epoch 7\n",
      "\n",
      "Classification Report for Fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      type-1       0.33      1.00      0.50         1\n",
      "      type-2       1.00      0.40      0.57         5\n",
      "      type-3       0.00      0.00      0.00         2\n",
      "      type-4       0.80      1.00      0.89         8\n",
      "      type-5       0.50      0.25      0.33         4\n",
      "      type-6       0.62      0.83      0.71         6\n",
      "      type-7       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           0.76        37\n",
      "   macro avg       0.61      0.64      0.57        37\n",
      "weighted avg       0.77      0.76      0.73        37\n",
      "\n",
      "======= Fold 2 =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 1/20 - Train Loss: 1.3986, Train Acc: 0.2276 - Val Loss: 1.3529, Val Acc: 0.2500\n",
      "Fold 2, Epoch 2/20 - Train Loss: 1.3113, Train Acc: 0.3172 - Val Loss: 1.2609, Val Acc: 0.3611\n",
      "Fold 2, Epoch 3/20 - Train Loss: 1.1733, Train Acc: 0.4897 - Val Loss: 1.1020, Val Acc: 0.4444\n",
      "Fold 2, Epoch 4/20 - Train Loss: 1.0684, Train Acc: 0.5103 - Val Loss: 0.8978, Val Acc: 0.4444\n",
      "Fold 2, Epoch 5/20 - Train Loss: 0.8869, Train Acc: 0.6000 - Val Loss: 0.7227, Val Acc: 0.5556\n",
      "Fold 2, Epoch 6/20 - Train Loss: 0.7931, Train Acc: 0.5379 - Val Loss: 0.5694, Val Acc: 0.6389\n",
      "Fold 2, Epoch 7/20 - Train Loss: 0.6368, Train Acc: 0.6759 - Val Loss: 0.5024, Val Acc: 0.6944\n",
      "Fold 2, Epoch 8/20 - Train Loss: 0.5630, Train Acc: 0.6759 - Val Loss: 0.4310, Val Acc: 0.6667\n",
      "Fold 2, Epoch 9/20 - Train Loss: 0.5117, Train Acc: 0.6759 - Val Loss: 0.4832, Val Acc: 0.6389\n",
      "Fold 2, Epoch 10/20 - Train Loss: 0.4473, Train Acc: 0.7586 - Val Loss: 0.4938, Val Acc: 0.6111\n",
      "Early stopping at epoch 10\n",
      "\n",
      "Classification Report for Fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      type-1       0.25      0.50      0.33         2\n",
      "      type-2       0.33      0.33      0.33         3\n",
      "      type-3       0.57      0.80      0.67         5\n",
      "      type-4       0.33      0.25      0.29         4\n",
      "      type-5       0.40      0.40      0.40         5\n",
      "      type-6       0.67      0.40      0.50         5\n",
      "      type-7       1.00      0.92      0.96        12\n",
      "\n",
      "    accuracy                           0.61        36\n",
      "   macro avg       0.51      0.51      0.50        36\n",
      "weighted avg       0.64      0.61      0.61        36\n",
      "\n",
      "======= Fold 3 =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 1/20 - Train Loss: 1.4228, Train Acc: 0.1586 - Val Loss: 1.3651, Val Acc: 0.3889\n",
      "Fold 3, Epoch 2/20 - Train Loss: 1.3085, Train Acc: 0.3379 - Val Loss: 1.2560, Val Acc: 0.3889\n",
      "Fold 3, Epoch 3/20 - Train Loss: 1.2477, Train Acc: 0.4138 - Val Loss: 1.1360, Val Acc: 0.6389\n",
      "Fold 3, Epoch 4/20 - Train Loss: 1.1176, Train Acc: 0.4966 - Val Loss: 0.9524, Val Acc: 0.6389\n",
      "Fold 3, Epoch 5/20 - Train Loss: 0.9071, Train Acc: 0.5862 - Val Loss: 0.7884, Val Acc: 0.5833\n",
      "Fold 3, Epoch 6/20 - Train Loss: 0.7782, Train Acc: 0.5655 - Val Loss: 0.6192, Val Acc: 0.7778\n",
      "Fold 3, Epoch 7/20 - Train Loss: 0.6410, Train Acc: 0.6828 - Val Loss: 0.6300, Val Acc: 0.6389\n",
      "Fold 3, Epoch 8/20 - Train Loss: 0.4789, Train Acc: 0.7310 - Val Loss: 0.5351, Val Acc: 0.6389\n",
      "Fold 3, Epoch 9/20 - Train Loss: 0.4803, Train Acc: 0.7172 - Val Loss: 0.5335, Val Acc: 0.7500\n",
      "Early stopping at epoch 9\n",
      "\n",
      "Classification Report for Fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      type-1       0.33      0.33      0.33         3\n",
      "      type-2       0.33      1.00      0.50         1\n",
      "      type-3       0.67      0.50      0.57         4\n",
      "      type-4       0.67      0.80      0.73         5\n",
      "      type-5       1.00      0.57      0.73         7\n",
      "      type-6       0.80      1.00      0.89         4\n",
      "      type-7       0.92      0.92      0.92        12\n",
      "\n",
      "    accuracy                           0.75        36\n",
      "   macro avg       0.67      0.73      0.67        36\n",
      "weighted avg       0.79      0.75      0.75        36\n",
      "\n",
      "======= Fold 4 =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 1/20 - Train Loss: 1.4235, Train Acc: 0.1793 - Val Loss: 1.3611, Val Acc: 0.3056\n",
      "Fold 4, Epoch 2/20 - Train Loss: 1.3388, Train Acc: 0.3931 - Val Loss: 1.2873, Val Acc: 0.5556\n",
      "Fold 4, Epoch 3/20 - Train Loss: 1.2244, Train Acc: 0.5310 - Val Loss: 1.1810, Val Acc: 0.6667\n",
      "Fold 4, Epoch 4/20 - Train Loss: 1.0826, Train Acc: 0.5586 - Val Loss: 1.0372, Val Acc: 0.6667\n",
      "Fold 4, Epoch 5/20 - Train Loss: 0.9187, Train Acc: 0.5517 - Val Loss: 0.9071, Val Acc: 0.5278\n",
      "Fold 4, Epoch 6/20 - Train Loss: 0.7750, Train Acc: 0.6138 - Val Loss: 0.7379, Val Acc: 0.6389\n",
      "Early stopping at epoch 6\n",
      "\n",
      "Classification Report for Fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      type-1       0.40      0.50      0.44         4\n",
      "      type-2       0.20      0.50      0.29         2\n",
      "      type-3       0.33      0.67      0.44         3\n",
      "      type-4       1.00      0.40      0.57         5\n",
      "      type-5       0.80      0.57      0.67         7\n",
      "      type-6       0.75      1.00      0.86         3\n",
      "      type-7       1.00      0.75      0.86        12\n",
      "\n",
      "    accuracy                           0.64        36\n",
      "   macro avg       0.64      0.63      0.59        36\n",
      "weighted avg       0.77      0.64      0.67        36\n",
      "\n",
      "======= Fold 5 =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 1/20 - Train Loss: 1.3691, Train Acc: 0.2414 - Val Loss: 1.2152, Val Acc: 0.5556\n",
      "Fold 5, Epoch 2/20 - Train Loss: 1.2807, Train Acc: 0.3655 - Val Loss: 1.0594, Val Acc: 0.6389\n",
      "Fold 5, Epoch 3/20 - Train Loss: 1.1597, Train Acc: 0.4207 - Val Loss: 0.8640, Val Acc: 0.7222\n",
      "Fold 5, Epoch 4/20 - Train Loss: 0.9830, Train Acc: 0.5793 - Val Loss: 0.7263, Val Acc: 0.7500\n",
      "Fold 5, Epoch 5/20 - Train Loss: 0.8691, Train Acc: 0.5862 - Val Loss: 0.6467, Val Acc: 0.6944\n",
      "Fold 5, Epoch 6/20 - Train Loss: 0.7272, Train Acc: 0.6138 - Val Loss: 0.5472, Val Acc: 0.7500\n",
      "Fold 5, Epoch 7/20 - Train Loss: 0.5871, Train Acc: 0.7310 - Val Loss: 0.5147, Val Acc: 0.6667\n",
      "Early stopping at epoch 7\n",
      "\n",
      "Classification Report for Fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      type-1       0.25      1.00      0.40         1\n",
      "      type-2       0.50      0.33      0.40         3\n",
      "      type-3       0.20      1.00      0.33         1\n",
      "      type-4       1.00      0.43      0.60         7\n",
      "      type-5       0.75      0.60      0.67         5\n",
      "      type-6       0.71      0.71      0.71         7\n",
      "      type-7       0.91      0.83      0.87        12\n",
      "\n",
      "    accuracy                           0.67        36\n",
      "   macro avg       0.62      0.70      0.57        36\n",
      "weighted avg       0.79      0.67      0.69        36\n",
      "\n",
      "\n",
      "Fold Accuracies: [0.7567567567567568, 0.6944444444444444, 0.7777777777777778, 0.6666666666666666, 0.75]\n",
      "Mean Accuracy: 0.7291291291291291\n"
     ]
    }
   ],
   "source": [
    "# Prepare full dataset indices for k-fold\n",
    "full_dataset = StoolDataset(DATA_DIR, transform=None)\n",
    "indices = list(range(len(full_dataset)))\n",
    "\n",
    "# Calculate class weights for full dataset\n",
    "all_labels_full = [label for _, label in full_dataset]\n",
    "class_counts = np.bincount(all_labels_full)\n",
    "class_weights = 1.0 / class_counts\n",
    "weights_full = [class_weights[label] for label in all_labels_full]\n",
    "\n",
    "kf = KFold(n_splits=KFOLDS, shuffle=True, random_state=SEED)\n",
    "fold_models = []\n",
    "fold_accuracies = []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kf.split(indices), 1):\n",
    "    print(f\"======= Fold {fold_idx} =======\")\n",
    "    # Subset transforms\n",
    "    train_ds = torch.utils.data.Subset(StoolDataset(DATA_DIR, transform=train_transforms), train_idx)\n",
    "    val_ds = torch.utils.data.Subset(StoolDataset(DATA_DIR, transform=val_transforms), val_idx)\n",
    "\n",
    "    # Create weighted sampler for train_ds\n",
    "    train_labels_fold = [train_ds.dataset.samples[i][1] for i in train_idx]\n",
    "    class_sample_count_fold = np.array([train_labels_fold.count(i) for i in range(NUM_CLASSES)])\n",
    "    class_weights_fold = 1.0 / class_sample_count_fold\n",
    "    sample_weights_fold = np.array([class_weights_fold[label] for label in train_labels_fold])\n",
    "    sample_weights_fold = torch.from_numpy(sample_weights_fold.astype(np.double))\n",
    "    sampler_fold = WeightedRandomSampler(sample_weights_fold, num_samples=len(sample_weights_fold), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler_fold)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Create model and freeze initial layers\n",
    "    model = create_model(backbone='efficientnet_b0')\n",
    "    # Optionally freeze until a certain layer name, e.g., 'features.4'\n",
    "    # model = create_model(backbone='efficientnet_b0', freeze_until_layer='features.4')\n",
    "\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n",
    "    # Choose loss: label smoothing or focal\n",
    "    # criterion = criterion_smooth\n",
    "    criterion = FocalLoss(alpha=1, gamma=2)\n",
    "\n",
    "    # Train and validate\n",
    "    best_model, best_acc = train_validate(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, fold_idx)\n",
    "    fold_models.append(best_model)\n",
    "    fold_accuracies.append(best_acc)\n",
    "\n",
    "# Summary of fold accuracies\n",
    "print(\"\\nFold Accuracies:\", fold_accuracies)\n",
    "print(\"Mean Accuracy:\", np.mean(fold_accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffe743fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Ensemble inference on a test image\n",
    "def ensemble_predict(models, image_path, transform):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    img_t = transform(image).unsqueeze(0).to(DEVICE)\n",
    "    probs = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(img_t)\n",
    "            probs.append(F.softmax(outputs, dim=1).cpu().numpy())\n",
    "    avg_probs = np.mean(np.vstack(probs), axis=0)\n",
    "    pred_class = np.argmax(avg_probs)\n",
    "    return sorted(os.listdir(DATA_DIR))[pred_class], np.max(avg_probs)\n",
    "\n",
    "# Usage example (replace 'some_image.jpg')\n",
    "# label, confidence = ensemble_predict(fold_models, 'some_image.jpg', val_transforms)\n",
    "# print(f\"Ensembled Prediction: {label}, Confidence: {confidence:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
