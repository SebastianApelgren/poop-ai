{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bc1ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, Subset\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tabulate import tabulate\n",
    "\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import OneCycleLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62f7fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a97ce0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_confusion_matrix(cm):\n",
    "    \"\"\"Returns a string of a nicely formatted confusion matrix with indices and highlighted diagonal.\"\"\"\n",
    "    headers = [\"\"] + [f\"Pred {i}\" for i in range(len(cm[0]))]\n",
    "    table = []\n",
    "\n",
    "    for i, row in enumerate(cm):\n",
    "        formatted_row = []\n",
    "        for j, val in enumerate(row):\n",
    "            if i == j:\n",
    "                formatted_row.append(f\"*{val}*\")  # Highlight diagonal\n",
    "            else:\n",
    "                formatted_row.append(str(val))\n",
    "        table.append([f\"True {i}\"] + formatted_row)\n",
    "\n",
    "    return tabulate(table, headers=headers, tablefmt=\"grid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b14f9e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logs = []\n",
    "\n",
    "def log_and_store(*msgs, table_format=False, is_confmat=False):\n",
    "    \"\"\"\n",
    "    Logs plain messages or pretty-prints confusion matrices or tables.\n",
    "    \"\"\"\n",
    "    if is_confmat and len(msgs) == 1 and isinstance(msgs[0], list):\n",
    "        msg = format_confusion_matrix(msgs[0])\n",
    "    elif table_format and len(msgs) == 1 and isinstance(msgs[0], list):\n",
    "        msg = tabulate(msgs[0], tablefmt=\"grid\")\n",
    "    else:\n",
    "        msg = \" \".join(str(m) for m in msgs)\n",
    "\n",
    "    print(msg)\n",
    "    all_logs.append(msg)\n",
    "\n",
    "def get_logs():\n",
    "    \"\"\"\n",
    "    Returnerar en lista med alla loggade meddelanden.\n",
    "    \"\"\"\n",
    "    return all_logs\n",
    "\n",
    "def clear_logs():\n",
    "    \"\"\"\n",
    "    Tömmer loggen.\n",
    "    \"\"\"\n",
    "    all_logs.clear()\n",
    "\n",
    "def save_logs_to_file(filename):\n",
    "    \"\"\"\n",
    "    Sparar loggade meddelanden till en fil.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        for log in all_logs:\n",
    "            f.write(log + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79638fe9",
   "metadata": {},
   "source": [
    "# Advanced Training Pipeline\n",
    "\n",
    "This notebook implements:\n",
    "1. Stronger and more varied augmentation, including class-specific oversampling.\n",
    "2. Model-level adjustments: gradual unfreezing, EfficientNet-B0/B3, label smoothing, focal loss/class-weighted loss.\n",
    "3. Training strategies: early stopping, checkpoint ensembles, and k-fold cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea9da62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = \"../datasets/data-BSS\"  # Update this path\n",
    "IMG_SIZE = 224\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "NUM_CLASSES = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd323500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoolDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.class_to_idx = {}\n",
    "        for idx, class_name in enumerate(sorted(os.listdir(root_dir))):\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                self.class_to_idx[class_name] = idx\n",
    "                for fname in os.listdir(class_path):\n",
    "                    if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        self.samples.append((os.path.join(class_path, fname), idx))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "481bd449",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),  # random crop + resize\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(3)], p=0.2),\n",
    "    transforms.RandomApply([transforms.Lambda(lambda img: img.filter(ImageFilter.FIND_EDGES))], p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "340ab4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Smoothing Loss (CrossEntropy with label_smoothing)\n",
    "criterion_smooth = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Focal Loss Implementation\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * ((1 - pt) ** self.gamma) * ce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "        \n",
    "    def __name__(self):\n",
    "        return \"FocalLoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f3691ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef create_model(backbone='efficientnet_b0', num_classes=NUM_CLASSES, freeze_until_layer=None):\\n    # Load pretrained EfficientNet\\n    if backbone == 'efficientnet_b0':\\n        model = models.efficientnet_b0(pretrained=True)\\n    elif backbone == 'efficientnet_b3':\\n        model = models.efficientnet_b3(pretrained=True)\\n    elif backbone == 'mobilenet_v2':\\n        model = models.mobilenet_v2(pretrained=True)\\n    elif backbone == 'mobilenet_v3_small':\\n        model = models.mobilenet_v3_small(pretrained=True)\\n    else:\\n        raise ValueError('Invalid backbone')\\n\\n    # Replace classifier head\\n    in_features = model.classifier[1].in_features\\n    model.classifier = nn.Sequential(\\n        nn.Linear(in_features, 512),\\n        nn.ReLU(inplace=True),\\n        nn.Dropout(0.4),\\n        nn.Linear(512, num_classes)\\n    )\\n\\n    # Freeze layers if specified\\n    if freeze_until_layer:\\n        for name, param in model.named_parameters():\\n            param.requires_grad = False\\n            if name.startswith(freeze_until_layer):\\n                break\\n        # Unfreeze subsequent layers\\n        unfreeze = False\\n        for name, param in model.named_parameters():\\n            if unfreeze:\\n                param.requires_grad = True\\n            if name.startswith(freeze_until_layer):\\n                unfreeze = True\\n\\n    return model.to(DEVICE)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def create_model(backbone='efficientnet_b0', num_classes=NUM_CLASSES, freeze_until_layer=None):\n",
    "    # Load pretrained EfficientNet\n",
    "    if backbone == 'efficientnet_b0':\n",
    "        model = models.efficientnet_b0(pretrained=True)\n",
    "    elif backbone == 'efficientnet_b3':\n",
    "        model = models.efficientnet_b3(pretrained=True)\n",
    "    elif backbone == 'mobilenet_v2':\n",
    "        model = models.mobilenet_v2(pretrained=True)\n",
    "    elif backbone == 'mobilenet_v3_small':\n",
    "        model = models.mobilenet_v3_small(pretrained=True)\n",
    "    else:\n",
    "        raise ValueError('Invalid backbone')\n",
    "\n",
    "    # Replace classifier head\n",
    "    in_features = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(in_features, 512),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    \n",
    "    # Freeze layers if specified\n",
    "    if freeze_until_layer:\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "            if name.startswith(freeze_until_layer):\n",
    "                break\n",
    "        # Unfreeze subsequent layers\n",
    "        unfreeze = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if unfreeze:\n",
    "                param.requires_grad = True\n",
    "            if name.startswith(freeze_until_layer):\n",
    "                unfreeze = True\n",
    "    \n",
    "    return model.to(DEVICE)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5c33c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(backbone, num_classes=NUM_CLASSES, freeze_until_layer=None):\n",
    "    if backbone == 'mobilenet_v3_small':\n",
    "        model = models.mobilenet_v3_small(pretrained=True)\n",
    "        # freeze layers\n",
    "        if freeze_until_layer:\n",
    "            for name, param in model.features.named_parameters():\n",
    "                param.requires_grad = False\n",
    "                if freeze_until_layer in name:\n",
    "                    break\n",
    "\n",
    "        # Replace final classifier\n",
    "        in_features = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    elif backbone == 'mobilenet_v3_large':\n",
    "        model = models.mobilenet_v3_large(pretrained=True)\n",
    "        if freeze_until_layer:\n",
    "            for name, param in model.features.named_parameters():\n",
    "                param.requires_grad = False\n",
    "                if freeze_until_layer in name:\n",
    "                    break\n",
    "\n",
    "        in_features = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    elif backbone == 'mobilenet_v2':\n",
    "        model = models.mobilenet_v2(pretrained=True)\n",
    "        if freeze_until_layer:\n",
    "            for name, param in model.features.named_parameters():\n",
    "                param.requires_grad = False\n",
    "                if freeze_until_layer in name:\n",
    "                    break\n",
    "\n",
    "        in_features = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    elif backbone == 'efficientnet_b0':\n",
    "        model = models.efficientnet_b0(pretrained=True)\n",
    "        if freeze_until_layer:\n",
    "            for name, param in model.features.named_parameters():\n",
    "                param.requires_grad = False\n",
    "                if freeze_until_layer in name:\n",
    "                    break\n",
    "\n",
    "        in_features = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    elif backbone == 'efficientnet_b3':\n",
    "        model = models.efficientnet_b3(pretrained=True)\n",
    "        if freeze_until_layer:\n",
    "            for name, param in model.features.named_parameters():\n",
    "                param.requires_grad = False\n",
    "                if freeze_until_layer in name:\n",
    "                    break\n",
    "\n",
    "        in_features = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Invalid backbone')\n",
    "\n",
    "    return model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb02600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return None, None, all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84c606df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate(model, train_loader, val_loader, criterion, optimizer, num_epochs, fold_idx):\n",
    "\n",
    "    #patience = 3\n",
    "    #counter = 0\n",
    "    #lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3) # for accuracy\n",
    "    #lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3) # for loss\n",
    "\n",
    "    best_acc     = 0.0\n",
    "    best_loss    = float('inf')\n",
    "    best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # ── One‐Cycle LR schedule ──────────────────────────────────────────────────\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=optimizer.param_groups[0]['lr'] * 10,  # e.g. 10× your base LR\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0) \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = running_corrects / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        val_total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_corrects += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_loss_epoch = val_loss / val_total\n",
    "        val_acc_epoch = val_corrects / val_total\n",
    "        # step the one‐cycle scheduler each batch‐cycle (done at epoch‐end here)\n",
    "        scheduler.step()\n",
    "\n",
    "        # keep snapshot of best‐ever validation loss (for final restore)\n",
    "        if val_loss_epoch < best_loss:\n",
    "            best_loss    = val_loss_epoch\n",
    "            best_acc     = val_acc_epoch\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print(f\"Fold {fold_idx}, Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f} - \"\n",
    "              f\"Val Loss: {val_loss_epoch:.4f}, Val Acc: {val_acc_epoch:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        #if val_acc_epoch > best_acc:\n",
    "        #    best_acc = val_acc_epoch\n",
    "        #   best_weights = model.state_dict().copy()\n",
    "        #   counter = 0\n",
    "        #else:\n",
    "        #    counter += 1\n",
    "        #    if counter >= patience:\n",
    "        #        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        #       break\n",
    "\n",
    "    # Load best weights\n",
    "    model.load_state_dict(best_weights)\n",
    "\n",
    "    # Final validation metrics\n",
    "    _, _, preds, labels = evaluate_model(model, val_loader)\n",
    "    log_and_store(\"\\nClassification Report for Fold {}:\".format(fold_idx))\n",
    "    log_and_store(classification_report(labels, preds, target_names=sorted(os.listdir(DATA_DIR))))\n",
    "\n",
    "    return model, best_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a45e2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find ../datasets/data-BSS -name \".DS_Store\" -type f -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a1e6881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Choose device\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() \n",
    "    else \"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() \n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469cf9c6",
   "metadata": {},
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c234e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold_training(\n",
    "    data_dir,\n",
    "    backbone='mobilenet_v2',\n",
    "    freeze_until_layer=None,\n",
    "    criterion_fn=None,\n",
    "    num_epochs=10,\n",
    "    lr=1e-4,\n",
    "    k_folds=3,\n",
    "    batch_size=32,\n",
    "    train_transforms=None,\n",
    "    val_transforms=None,\n",
    "    seed=42,\n",
    "    num_classes=7,\n",
    "):\n",
    "    full_dataset = StoolDataset(data_dir, transform=None)\n",
    "    indices = list(range(len(full_dataset)))\n",
    "\n",
    "    # Class weights for full dataset (optional, for balance insights)\n",
    "    all_labels_full = [label for _, label in full_dataset]\n",
    "    class_counts = np.bincount(all_labels_full)\n",
    "    class_weights = 1.0 / class_counts\n",
    "    weights_full = [class_weights[label] for label in all_labels_full]\n",
    "\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
    "    fold_models = []\n",
    "    fold_accuracies = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(indices), 1):\n",
    "        print(f\"\\n======= Fold {fold_idx} =======\")\n",
    "\n",
    "        # Subset + transforms\n",
    "        train_ds = torch.utils.data.Subset(StoolDataset(data_dir, transform=train_transforms), train_idx)\n",
    "        val_ds   = torch.utils.data.Subset(StoolDataset(data_dir, transform=val_transforms), val_idx)\n",
    "\n",
    "        # Weighted sampler\n",
    "        train_labels_fold = [train_ds.dataset.samples[i][1] for i in train_idx]\n",
    "        class_sample_count_fold = np.array([train_labels_fold.count(i) for i in range(num_classes)])\n",
    "        print(f\"Class sample counts: {class_sample_count_fold}\")\n",
    "        class_weights_fold = 1.0 / class_sample_count_fold\n",
    "        sample_weights_fold = np.array([class_weights_fold[label] for label in train_labels_fold])\n",
    "        sample_weights_fold = torch.from_numpy(sample_weights_fold.astype(np.double))\n",
    "        sampler_fold = WeightedRandomSampler(sample_weights_fold, num_samples=len(sample_weights_fold), replacement=True)\n",
    "\n",
    "        # DataLoaders\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler_fold)\n",
    "        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Create model\n",
    "        model = create_model(backbone=backbone, freeze_until_layer=freeze_until_layer)\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "        # Loss\n",
    "        criterion = criterion_fn if criterion_fn is not None else nn.CrossEntropyLoss()\n",
    "\n",
    "        # Train\n",
    "        best_model, best_acc = train_validate(model, train_loader, val_loader, criterion, optimizer, num_epochs, fold_idx)\n",
    "        fold_models.append(best_model)\n",
    "        fold_accuracies.append(best_acc)\n",
    "\n",
    "        # Evaluate and log\n",
    "        best_model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                logits = best_model(xb)\n",
    "                preds = logits.argmax(dim=1).cpu().numpy()\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(yb.numpy())\n",
    "\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        crpt = classification_report(all_labels, all_preds, digits=4)\n",
    "\n",
    "        log_and_store(f\"\\n--- Fold {fold_idx} Confusion Matrix ---\")\n",
    "        log_and_store(cm, is_confmat=True)\n",
    "\n",
    "        log_and_store(f\"\\n--- Fold {fold_idx} Classification Report ---\")\n",
    "        log_and_store(crpt)\n",
    "\n",
    "    log_and_store([\"\\nFold Models:\", [f\"Fold {i+1}\" for i in range(len(fold_models))]])\n",
    "    log_and_store([\"Fold Accuracies:\", fold_accuracies])\n",
    "    log_and_store([\"Mean Accuracy:\", np.mean(fold_accuracies)])\n",
    "\n",
    "    return fold_models, fold_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ae6bb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_split(\n",
    "    data_dir,\n",
    "    model_name='efficientnet_b3',\n",
    "    freeze_until=None,\n",
    "    criterion='focal',  # or 'smooth'\n",
    "    batch_size=32,\n",
    "    lr=1e-4,\n",
    "    num_epochs=10,\n",
    "    val_split=0.2,\n",
    "    seed=42,\n",
    "):\n",
    "    print(\"======= Single Split Training =======\")\n",
    "\n",
    "    # Full dataset\n",
    "    full_dataset = StoolDataset(data_dir, transform=None)\n",
    "    indices = list(range(len(full_dataset)))\n",
    "    all_labels = [label for _, label in full_dataset]\n",
    "\n",
    "    # Train/val split\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        indices, test_size=val_split, random_state=seed, stratify=all_labels\n",
    "    )\n",
    "\n",
    "    # Create datasets with transforms\n",
    "    train_ds = Subset(StoolDataset(data_dir, transform=train_transforms), train_idx)\n",
    "    val_ds = Subset(StoolDataset(data_dir, transform=val_transforms), val_idx)\n",
    "\n",
    "    # Weighted sampler for class imbalance\n",
    "    train_labels = [train_ds.dataset.samples[i][1] for i in train_idx]\n",
    "    class_sample_counts = np.array([train_labels.count(i) for i in range(NUM_CLASSES)])\n",
    "    print(f\"Class sample counts: {class_sample_counts}\")\n",
    "    class_weights = 1.0 / class_sample_counts\n",
    "    sample_weights = np.array([class_weights[label] for label in train_labels])\n",
    "    sample_weights = torch.from_numpy(sample_weights.astype(np.double))\n",
    "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    model = create_model(backbone=model_name, freeze_until_layer=freeze_until)\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "    if criterion == 'focal':\n",
    "        loss_fn = FocalLoss(alpha=1, gamma=2)\n",
    "    elif criterion == 'smooth':\n",
    "        loss_fn = criterion_smooth\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported loss function\")\n",
    "\n",
    "    # Train\n",
    "    best_model, best_acc = train_validate(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, fold_idx=None)\n",
    "\n",
    "    # Evaluation\n",
    "    best_model.eval()\n",
    "    all_preds, all_labels_eval = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            preds = best_model(xb).argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels_eval.extend(yb.numpy())\n",
    "\n",
    "    # Confusion matrix and classification report\n",
    "    cm = confusion_matrix(all_labels_eval, all_preds)\n",
    "    cr = classification_report(all_labels_eval, all_preds, digits=4)\n",
    "\n",
    "    log_and_store(\"--- Confusion Matrix ---\")\n",
    "    log_and_store(cm, is_confmat=True)\n",
    "    log_and_store(\"--- Classification Report ---\")\n",
    "    log_and_store(cr)\n",
    "\n",
    "    return best_model, best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19280109",
   "metadata": {},
   "source": [
    "# Training ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b24a86c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_logs()  # Clear logs if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d5b2aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "backbone = 'efficientnet_b3'  # or 'mobilenet_v2', 'mobilenet_v3_small', efficientnet_b3, efficientnet_b0.\n",
    "freeze_until = 'features.4'  # e.g., 'features.4'\n",
    "criterion = 'focal' # 'focal' or 'smooth'\n",
    "num_epochs = 20\n",
    "batch_size = 16 # This represents the batch size for training and validation which is the number of samples processed before the model is updated.\n",
    "lr = 1e-4  # Learning rate\n",
    "\n",
    "val_split = 0.2  # Fraction of data to use for validation\n",
    "\n",
    "k_folds = 3\n",
    "seed = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f938440b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Fold 1 =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1/25 - Train Loss: 1.3894, Train Acc: 0.1985 - Val Loss: 1.3430, Val Acc: 0.2938\n",
      "Fold 1, Epoch 2/25 - Train Loss: 1.3055, Train Acc: 0.3041 - Val Loss: 1.2449, Val Acc: 0.3918\n",
      "Fold 1, Epoch 3/25 - Train Loss: 1.1855, Train Acc: 0.3814 - Val Loss: 1.1397, Val Acc: 0.4639\n",
      "Fold 1, Epoch 4/25 - Train Loss: 1.0984, Train Acc: 0.4446 - Val Loss: 1.0660, Val Acc: 0.4536\n",
      "Fold 1, Epoch 5/25 - Train Loss: 1.0050, Train Acc: 0.5013 - Val Loss: 1.0056, Val Acc: 0.4691\n",
      "Fold 1, Epoch 6/25 - Train Loss: 0.9427, Train Acc: 0.5155 - Val Loss: 0.9152, Val Acc: 0.4691\n",
      "Fold 1, Epoch 7/25 - Train Loss: 0.8515, Train Acc: 0.5451 - Val Loss: 0.8592, Val Acc: 0.4330\n",
      "Fold 1, Epoch 8/25 - Train Loss: 0.7587, Train Acc: 0.5851 - Val Loss: 0.7459, Val Acc: 0.5309\n",
      "Fold 1, Epoch 9/25 - Train Loss: 0.6569, Train Acc: 0.6469 - Val Loss: 0.7472, Val Acc: 0.5361\n",
      "Fold 1, Epoch 10/25 - Train Loss: 0.5770, Train Acc: 0.6856 - Val Loss: 0.7018, Val Acc: 0.5206\n",
      "Fold 1, Epoch 11/25 - Train Loss: 0.5606, Train Acc: 0.6714 - Val Loss: 0.6128, Val Acc: 0.5258\n",
      "Fold 1, Epoch 12/25 - Train Loss: 0.4787, Train Acc: 0.7062 - Val Loss: 0.6088, Val Acc: 0.5979\n",
      "Fold 1, Epoch 13/25 - Train Loss: 0.4702, Train Acc: 0.7088 - Val Loss: 0.5792, Val Acc: 0.5722\n",
      "Fold 1, Epoch 14/25 - Train Loss: 0.4466, Train Acc: 0.7320 - Val Loss: 0.5777, Val Acc: 0.5567\n",
      "Fold 1, Epoch 15/25 - Train Loss: 0.4110, Train Acc: 0.7204 - Val Loss: 0.5718, Val Acc: 0.5773\n",
      "Fold 1, Epoch 16/25 - Train Loss: 0.3842, Train Acc: 0.7680 - Val Loss: 0.5601, Val Acc: 0.5928\n",
      "Fold 1, Epoch 17/25 - Train Loss: 0.3558, Train Acc: 0.7874 - Val Loss: 0.5341, Val Acc: 0.5773\n",
      "Fold 1, Epoch 18/25 - Train Loss: 0.3500, Train Acc: 0.7680 - Val Loss: 0.5120, Val Acc: 0.5773\n",
      "Fold 1, Epoch 19/25 - Train Loss: 0.3058, Train Acc: 0.8015 - Val Loss: 0.5336, Val Acc: 0.5773\n",
      "Fold 1, Epoch 20/25 - Train Loss: 0.2921, Train Acc: 0.8028 - Val Loss: 0.5476, Val Acc: 0.5825\n",
      "Fold 1, Epoch 21/25 - Train Loss: 0.2494, Train Acc: 0.8196 - Val Loss: 0.5439, Val Acc: 0.6134\n",
      "Fold 1, Epoch 22/25 - Train Loss: 0.2371, Train Acc: 0.8260 - Val Loss: 0.5108, Val Acc: 0.6031\n",
      "Fold 1, Epoch 23/25 - Train Loss: 0.2326, Train Acc: 0.8466 - Val Loss: 0.5016, Val Acc: 0.5722\n",
      "Fold 1, Epoch 24/25 - Train Loss: 0.2318, Train Acc: 0.8363 - Val Loss: 0.5452, Val Acc: 0.5773\n",
      "Fold 1, Epoch 25/25 - Train Loss: 0.2223, Train Acc: 0.8415 - Val Loss: 0.5488, Val Acc: 0.5979\n",
      "\n",
      "Classification Report for Fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      type-1       0.35      1.00      0.52         6\n",
      "      type-2       0.33      0.22      0.26        23\n",
      "      type-3       0.59      0.53      0.56        70\n",
      "      type-4       0.65      0.66      0.65        70\n",
      "      type-5       0.33      0.17      0.22         6\n",
      "      type-6       0.40      0.40      0.40         5\n",
      "      type-7       0.70      1.00      0.82        14\n",
      "\n",
      "    accuracy                           0.57       194\n",
      "   macro avg       0.48      0.57      0.49       194\n",
      "weighted avg       0.57      0.57      0.56       194\n",
      "\n",
      "\n",
      "--- Fold 1 Confusion Matrix ---\n",
      "[[ 6  0  0  0  0  0  0]\n",
      " [ 7  5  8  1  1  0  1]\n",
      " [ 3  6 37 22  1  0  1]\n",
      " [ 1  4 18 46  0  0  1]\n",
      " [ 0  0  0  2  1  3  0]\n",
      " [ 0  0  0  0  0  2  3]\n",
      " [ 0  0  0  0  0  0 14]]\n",
      "\n",
      "--- Fold 1 Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3529    1.0000    0.5217         6\n",
      "           1     0.3333    0.2174    0.2632        23\n",
      "           2     0.5873    0.5286    0.5564        70\n",
      "           3     0.6479    0.6571    0.6525        70\n",
      "           4     0.3333    0.1667    0.2222         6\n",
      "           5     0.4000    0.4000    0.4000         5\n",
      "           6     0.7000    1.0000    0.8235        14\n",
      "\n",
      "    accuracy                         0.5722       194\n",
      "   macro avg     0.4793    0.5671    0.4914       194\n",
      "weighted avg     0.5673    0.5722    0.5601       194\n",
      "\n",
      "\n",
      "======= Fold 2 =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 1/25 - Train Loss: 1.3934, Train Acc: 0.1946 - Val Loss: 1.3612, Val Acc: 0.2113\n",
      "Fold 2, Epoch 2/25 - Train Loss: 1.2941, Train Acc: 0.3222 - Val Loss: 1.3038, Val Acc: 0.3144\n",
      "Fold 2, Epoch 3/25 - Train Loss: 1.1913, Train Acc: 0.3737 - Val Loss: 1.2145, Val Acc: 0.3402\n",
      "Fold 2, Epoch 4/25 - Train Loss: 1.0722, Train Acc: 0.4704 - Val Loss: 1.1349, Val Acc: 0.3763\n",
      "Fold 2, Epoch 5/25 - Train Loss: 0.9851, Train Acc: 0.4897 - Val Loss: 1.0708, Val Acc: 0.4124\n",
      "Fold 2, Epoch 6/25 - Train Loss: 0.8655, Train Acc: 0.5464 - Val Loss: 0.9729, Val Acc: 0.4227\n",
      "Fold 2, Epoch 7/25 - Train Loss: 0.7838, Train Acc: 0.5799 - Val Loss: 0.8844, Val Acc: 0.4433\n",
      "Fold 2, Epoch 8/25 - Train Loss: 0.7271, Train Acc: 0.5941 - Val Loss: 0.8369, Val Acc: 0.5000\n",
      "Fold 2, Epoch 9/25 - Train Loss: 0.5993, Train Acc: 0.6856 - Val Loss: 0.8093, Val Acc: 0.4845\n",
      "Fold 2, Epoch 10/25 - Train Loss: 0.5646, Train Acc: 0.6572 - Val Loss: 0.7392, Val Acc: 0.5155\n",
      "Fold 2, Epoch 11/25 - Train Loss: 0.5343, Train Acc: 0.6894 - Val Loss: 0.7392, Val Acc: 0.5309\n",
      "Fold 2, Epoch 12/25 - Train Loss: 0.4574, Train Acc: 0.7268 - Val Loss: 0.7156, Val Acc: 0.5258\n",
      "Fold 2, Epoch 13/25 - Train Loss: 0.4165, Train Acc: 0.7448 - Val Loss: 0.7144, Val Acc: 0.5515\n",
      "Fold 2, Epoch 14/25 - Train Loss: 0.3891, Train Acc: 0.7771 - Val Loss: 0.7071, Val Acc: 0.5412\n",
      "Fold 2, Epoch 15/25 - Train Loss: 0.3770, Train Acc: 0.7668 - Val Loss: 0.6876, Val Acc: 0.5052\n",
      "Fold 2, Epoch 16/25 - Train Loss: 0.3480, Train Acc: 0.7758 - Val Loss: 0.6560, Val Acc: 0.5619\n",
      "Fold 2, Epoch 17/25 - Train Loss: 0.3258, Train Acc: 0.7874 - Val Loss: 0.6720, Val Acc: 0.5567\n",
      "Fold 2, Epoch 18/25 - Train Loss: 0.3250, Train Acc: 0.7848 - Val Loss: 0.6770, Val Acc: 0.5567\n",
      "Fold 2, Epoch 19/25 - Train Loss: 0.2662, Train Acc: 0.8209 - Val Loss: 0.6478, Val Acc: 0.5825\n",
      "Fold 2, Epoch 20/25 - Train Loss: 0.2850, Train Acc: 0.8054 - Val Loss: 0.6282, Val Acc: 0.5722\n",
      "Fold 2, Epoch 21/25 - Train Loss: 0.2429, Train Acc: 0.8363 - Val Loss: 0.6389, Val Acc: 0.5670\n",
      "Fold 2, Epoch 22/25 - Train Loss: 0.2108, Train Acc: 0.8454 - Val Loss: 0.6437, Val Acc: 0.5619\n",
      "Fold 2, Epoch 23/25 - Train Loss: 0.2437, Train Acc: 0.8312 - Val Loss: 0.6101, Val Acc: 0.5773\n",
      "Fold 2, Epoch 24/25 - Train Loss: 0.2016, Train Acc: 0.8608 - Val Loss: 0.6291, Val Acc: 0.5722\n",
      "Fold 2, Epoch 25/25 - Train Loss: 0.1962, Train Acc: 0.8505 - Val Loss: 0.6478, Val Acc: 0.5464\n",
      "\n",
      "Classification Report for Fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      type-1       0.64      0.64      0.64        14\n",
      "      type-2       0.22      0.33      0.26        15\n",
      "      type-3       0.53      0.48      0.50        64\n",
      "      type-4       0.69      0.69      0.69        64\n",
      "      type-5       0.67      0.40      0.50        10\n",
      "      type-6       0.56      0.56      0.56         9\n",
      "      type-7       0.74      0.78      0.76        18\n",
      "\n",
      "    accuracy                           0.58       194\n",
      "   macro avg       0.58      0.55      0.56       194\n",
      "weighted avg       0.59      0.58      0.58       194\n",
      "\n",
      "\n",
      "--- Fold 2 Confusion Matrix ---\n",
      "[[ 9  1  3  0  0  0  1]\n",
      " [ 2  5  6  1  0  0  1]\n",
      " [ 2 12 31 16  0  1  2]\n",
      " [ 1  3 16 44  0  0  0]\n",
      " [ 0  1  2  2  4  0  1]\n",
      " [ 0  0  1  1  2  5  0]\n",
      " [ 0  1  0  0  0  3 14]]\n",
      "\n",
      "--- Fold 2 Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6429    0.6429    0.6429        14\n",
      "           1     0.2174    0.3333    0.2632        15\n",
      "           2     0.5254    0.4844    0.5041        64\n",
      "           3     0.6875    0.6875    0.6875        64\n",
      "           4     0.6667    0.4000    0.5000        10\n",
      "           5     0.5556    0.5556    0.5556         9\n",
      "           6     0.7368    0.7778    0.7568        18\n",
      "\n",
      "    accuracy                         0.5773       194\n",
      "   macro avg     0.5760    0.5545    0.5586       194\n",
      "weighted avg     0.5918    0.5773    0.5816       194\n",
      "\n",
      "\n",
      "======= Fold 3 =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 1/25 - Train Loss: 1.4076, Train Acc: 0.1869 - Val Loss: 1.3993, Val Acc: 0.2113\n",
      "Fold 3, Epoch 2/25 - Train Loss: 1.3135, Train Acc: 0.2887 - Val Loss: 1.3324, Val Acc: 0.2577\n",
      "Fold 3, Epoch 3/25 - Train Loss: 1.2205, Train Acc: 0.3570 - Val Loss: 1.2609, Val Acc: 0.2990\n",
      "Fold 3, Epoch 4/25 - Train Loss: 1.1191, Train Acc: 0.4472 - Val Loss: 1.1531, Val Acc: 0.3711\n",
      "Fold 3, Epoch 5/25 - Train Loss: 1.0145, Train Acc: 0.4678 - Val Loss: 1.0613, Val Acc: 0.4588\n",
      "Fold 3, Epoch 6/25 - Train Loss: 0.9063, Train Acc: 0.5438 - Val Loss: 0.9748, Val Acc: 0.4845\n",
      "Fold 3, Epoch 7/25 - Train Loss: 0.8154, Train Acc: 0.5593 - Val Loss: 0.8847, Val Acc: 0.5155\n",
      "Fold 3, Epoch 8/25 - Train Loss: 0.7218, Train Acc: 0.5992 - Val Loss: 0.8389, Val Acc: 0.5052\n",
      "Fold 3, Epoch 9/25 - Train Loss: 0.6774, Train Acc: 0.6211 - Val Loss: 0.7805, Val Acc: 0.5619\n",
      "Fold 3, Epoch 10/25 - Train Loss: 0.5490, Train Acc: 0.6946 - Val Loss: 0.7393, Val Acc: 0.5206\n",
      "Fold 3, Epoch 11/25 - Train Loss: 0.5316, Train Acc: 0.6946 - Val Loss: 0.7700, Val Acc: 0.5000\n",
      "Fold 3, Epoch 12/25 - Train Loss: 0.4628, Train Acc: 0.7268 - Val Loss: 0.7448, Val Acc: 0.5258\n",
      "Fold 3, Epoch 13/25 - Train Loss: 0.4462, Train Acc: 0.7113 - Val Loss: 0.7418, Val Acc: 0.5103\n",
      "Fold 3, Epoch 14/25 - Train Loss: 0.4396, Train Acc: 0.7294 - Val Loss: 0.6841, Val Acc: 0.5361\n",
      "Fold 3, Epoch 15/25 - Train Loss: 0.3405, Train Acc: 0.7835 - Val Loss: 0.7109, Val Acc: 0.5309\n",
      "Fold 3, Epoch 16/25 - Train Loss: 0.3407, Train Acc: 0.7693 - Val Loss: 0.6778, Val Acc: 0.5155\n",
      "Fold 3, Epoch 17/25 - Train Loss: 0.3224, Train Acc: 0.7990 - Val Loss: 0.6617, Val Acc: 0.5309\n",
      "Fold 3, Epoch 18/25 - Train Loss: 0.2893, Train Acc: 0.8312 - Val Loss: 0.6831, Val Acc: 0.5309\n",
      "Fold 3, Epoch 19/25 - Train Loss: 0.2768, Train Acc: 0.8054 - Val Loss: 0.6560, Val Acc: 0.5412\n",
      "Fold 3, Epoch 20/25 - Train Loss: 0.2568, Train Acc: 0.8402 - Val Loss: 0.6957, Val Acc: 0.5258\n",
      "Fold 3, Epoch 21/25 - Train Loss: 0.2532, Train Acc: 0.8157 - Val Loss: 0.6354, Val Acc: 0.5567\n",
      "Fold 3, Epoch 22/25 - Train Loss: 0.2429, Train Acc: 0.8235 - Val Loss: 0.6517, Val Acc: 0.5000\n",
      "Fold 3, Epoch 23/25 - Train Loss: 0.2157, Train Acc: 0.8531 - Val Loss: 0.6297, Val Acc: 0.5567\n",
      "Fold 3, Epoch 24/25 - Train Loss: 0.2393, Train Acc: 0.8376 - Val Loss: 0.6756, Val Acc: 0.5258\n",
      "Fold 3, Epoch 25/25 - Train Loss: 0.2001, Train Acc: 0.8570 - Val Loss: 0.6583, Val Acc: 0.5361\n",
      "\n",
      "Classification Report for Fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      type-1       0.45      0.36      0.40        14\n",
      "      type-2       0.16      0.21      0.18        19\n",
      "      type-3       0.52      0.54      0.53        68\n",
      "      type-4       0.63      0.66      0.64        56\n",
      "      type-5       0.83      0.56      0.67         9\n",
      "      type-6       1.00      0.89      0.94         9\n",
      "      type-7       0.86      0.63      0.73        19\n",
      "\n",
      "    accuracy                           0.56       194\n",
      "   macro avg       0.64      0.55      0.58       194\n",
      "weighted avg       0.58      0.56      0.56       194\n",
      "\n",
      "\n",
      "--- Fold 3 Confusion Matrix ---\n",
      "[[ 5  5  3  0  0  0  1]\n",
      " [ 2  4  9  3  0  0  1]\n",
      " [ 1 13 37 16  1  0  0]\n",
      " [ 0  2 17 37  0  0  0]\n",
      " [ 2  0  0  2  5  0  0]\n",
      " [ 0  0  0  1  0  8  0]\n",
      " [ 1  1  5  0  0  0 12]]\n",
      "\n",
      "--- Fold 3 Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4545    0.3571    0.4000        14\n",
      "           1     0.1600    0.2105    0.1818        19\n",
      "           2     0.5211    0.5441    0.5324        68\n",
      "           3     0.6271    0.6607    0.6435        56\n",
      "           4     0.8333    0.5556    0.6667         9\n",
      "           5     1.0000    0.8889    0.9412         9\n",
      "           6     0.8571    0.6316    0.7273        19\n",
      "\n",
      "    accuracy                         0.5567       194\n",
      "   macro avg     0.6362    0.5498    0.5847       194\n",
      "weighted avg     0.5812    0.5567    0.5648       194\n",
      "\n",
      "\n",
      "======= Fold 4 =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 1/25 - Train Loss: 1.4073, Train Acc: 0.1830 - Val Loss: 1.3746, Val Acc: 0.2371\n",
      "Fold 4, Epoch 2/25 - Train Loss: 1.3196, Train Acc: 0.3080 - Val Loss: 1.2966, Val Acc: 0.3041\n",
      "Fold 4, Epoch 3/25 - Train Loss: 1.2162, Train Acc: 0.3943 - Val Loss: 1.2038, Val Acc: 0.3969\n",
      "Fold 4, Epoch 4/25 - Train Loss: 1.1193, Train Acc: 0.4291 - Val Loss: 1.1063, Val Acc: 0.4639\n",
      "Fold 4, Epoch 5/25 - Train Loss: 1.0135, Train Acc: 0.5103 - Val Loss: 0.9974, Val Acc: 0.5206\n",
      "Fold 4, Epoch 6/25 - Train Loss: 0.9175, Train Acc: 0.5322 - Val Loss: 0.9172, Val Acc: 0.5103\n",
      "Fold 4, Epoch 7/25 - Train Loss: 0.7946, Train Acc: 0.5644 - Val Loss: 0.8448, Val Acc: 0.5103\n",
      "Fold 4, Epoch 8/25 - Train Loss: 0.7251, Train Acc: 0.6044 - Val Loss: 0.7895, Val Acc: 0.5000\n",
      "Fold 4, Epoch 9/25 - Train Loss: 0.6530, Train Acc: 0.6198 - Val Loss: 0.7279, Val Acc: 0.5309\n",
      "Fold 4, Epoch 10/25 - Train Loss: 0.6013, Train Acc: 0.6637 - Val Loss: 0.7492, Val Acc: 0.5155\n",
      "Fold 4, Epoch 11/25 - Train Loss: 0.4801, Train Acc: 0.7113 - Val Loss: 0.6995, Val Acc: 0.5309\n",
      "Fold 4, Epoch 12/25 - Train Loss: 0.4908, Train Acc: 0.7126 - Val Loss: 0.6472, Val Acc: 0.5361\n",
      "Fold 4, Epoch 13/25 - Train Loss: 0.4396, Train Acc: 0.7229 - Val Loss: 0.6852, Val Acc: 0.5464\n",
      "Fold 4, Epoch 14/25 - Train Loss: 0.4034, Train Acc: 0.7448 - Val Loss: 0.6313, Val Acc: 0.5567\n",
      "Fold 4, Epoch 15/25 - Train Loss: 0.3565, Train Acc: 0.7474 - Val Loss: 0.6431, Val Acc: 0.5567\n",
      "Fold 4, Epoch 16/25 - Train Loss: 0.3412, Train Acc: 0.7577 - Val Loss: 0.6292, Val Acc: 0.5670\n",
      "Fold 4, Epoch 17/25 - Train Loss: 0.3358, Train Acc: 0.7784 - Val Loss: 0.6152, Val Acc: 0.5773\n",
      "Fold 4, Epoch 18/25 - Train Loss: 0.3267, Train Acc: 0.7951 - Val Loss: 0.6195, Val Acc: 0.5876\n",
      "Fold 4, Epoch 19/25 - Train Loss: 0.2811, Train Acc: 0.8041 - Val Loss: 0.6259, Val Acc: 0.5876\n",
      "Fold 4, Epoch 20/25 - Train Loss: 0.2481, Train Acc: 0.8312 - Val Loss: 0.6346, Val Acc: 0.5567\n",
      "Fold 4, Epoch 21/25 - Train Loss: 0.2660, Train Acc: 0.8067 - Val Loss: 0.6533, Val Acc: 0.5619\n",
      "Fold 4, Epoch 22/25 - Train Loss: 0.2509, Train Acc: 0.8428 - Val Loss: 0.6453, Val Acc: 0.5670\n",
      "Fold 4, Epoch 23/25 - Train Loss: 0.2319, Train Acc: 0.8428 - Val Loss: 0.6489, Val Acc: 0.5515\n",
      "Fold 4, Epoch 24/25 - Train Loss: 0.2059, Train Acc: 0.8557 - Val Loss: 0.6264, Val Acc: 0.5670\n",
      "Fold 4, Epoch 25/25 - Train Loss: 0.2190, Train Acc: 0.8441 - Val Loss: 0.6429, Val Acc: 0.5825\n",
      "\n",
      "Classification Report for Fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      type-1       0.48      0.77      0.59        13\n",
      "      type-2       0.26      0.26      0.26        19\n",
      "      type-3       0.57      0.47      0.51        64\n",
      "      type-4       0.72      0.72      0.72        61\n",
      "      type-5       0.36      0.44      0.40         9\n",
      "      type-6       0.57      0.67      0.62        12\n",
      "      type-7       0.73      0.69      0.71        16\n",
      "\n",
      "    accuracy                           0.58       194\n",
      "   macro avg       0.53      0.57      0.54       194\n",
      "weighted avg       0.58      0.58      0.58       194\n",
      "\n",
      "\n",
      "--- Fold 4 Confusion Matrix ---\n",
      "[[10  1  1  0  0  0  1]\n",
      " [ 4  5  5  1  2  0  2]\n",
      " [ 4 11 30 15  2  2  0]\n",
      " [ 0  0 13 44  2  1  1]\n",
      " [ 1  1  1  1  4  1  0]\n",
      " [ 0  0  3  0  1  8  0]\n",
      " [ 2  1  0  0  0  2 11]]\n",
      "\n",
      "--- Fold 4 Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4762    0.7692    0.5882        13\n",
      "           1     0.2632    0.2632    0.2632        19\n",
      "           2     0.5660    0.4688    0.5128        64\n",
      "           3     0.7213    0.7213    0.7213        61\n",
      "           4     0.3636    0.4444    0.4000         9\n",
      "           5     0.5714    0.6667    0.6154        12\n",
      "           6     0.7333    0.6875    0.7097        16\n",
      "\n",
      "    accuracy                         0.5773       194\n",
      "   macro avg     0.5279    0.5744    0.5444       194\n",
      "weighted avg     0.5839    0.5773    0.5763       194\n",
      "\n",
      "\n",
      "======= Fold 5 =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 1/25 - Train Loss: 1.4184, Train Acc: 0.1585 - Val Loss: 1.3740, Val Acc: 0.2887\n",
      "Fold 5, Epoch 2/25 - Train Loss: 1.2942, Train Acc: 0.3273 - Val Loss: 1.2893, Val Acc: 0.3866\n",
      "Fold 5, Epoch 3/25 - Train Loss: 1.2058, Train Acc: 0.4046 - Val Loss: 1.2039, Val Acc: 0.4639\n",
      "Fold 5, Epoch 4/25 - Train Loss: 1.1240, Train Acc: 0.4227 - Val Loss: 1.1243, Val Acc: 0.4536\n",
      "Fold 5, Epoch 5/25 - Train Loss: 1.0163, Train Acc: 0.4884 - Val Loss: 1.0244, Val Acc: 0.5309\n",
      "Fold 5, Epoch 6/25 - Train Loss: 0.9089, Train Acc: 0.5284 - Val Loss: 0.9524, Val Acc: 0.5103\n",
      "Fold 5, Epoch 7/25 - Train Loss: 0.8012, Train Acc: 0.6005 - Val Loss: 0.8396, Val Acc: 0.5515\n",
      "Fold 5, Epoch 8/25 - Train Loss: 0.7174, Train Acc: 0.6095 - Val Loss: 0.7604, Val Acc: 0.5619\n",
      "Fold 5, Epoch 9/25 - Train Loss: 0.6712, Train Acc: 0.6044 - Val Loss: 0.7484, Val Acc: 0.5876\n",
      "Fold 5, Epoch 10/25 - Train Loss: 0.5882, Train Acc: 0.6546 - Val Loss: 0.7024, Val Acc: 0.5876\n",
      "Fold 5, Epoch 11/25 - Train Loss: 0.5402, Train Acc: 0.6791 - Val Loss: 0.6649, Val Acc: 0.6031\n",
      "Fold 5, Epoch 12/25 - Train Loss: 0.5072, Train Acc: 0.6920 - Val Loss: 0.6225, Val Acc: 0.6031\n",
      "Fold 5, Epoch 13/25 - Train Loss: 0.4594, Train Acc: 0.7294 - Val Loss: 0.6016, Val Acc: 0.6186\n",
      "Fold 5, Epoch 14/25 - Train Loss: 0.3782, Train Acc: 0.7680 - Val Loss: 0.5884, Val Acc: 0.6082\n",
      "Fold 5, Epoch 15/25 - Train Loss: 0.3498, Train Acc: 0.7887 - Val Loss: 0.6418, Val Acc: 0.5825\n",
      "Fold 5, Epoch 16/25 - Train Loss: 0.3575, Train Acc: 0.7629 - Val Loss: 0.5857, Val Acc: 0.6082\n",
      "Fold 5, Epoch 17/25 - Train Loss: 0.3345, Train Acc: 0.7732 - Val Loss: 0.5876, Val Acc: 0.6392\n",
      "Fold 5, Epoch 18/25 - Train Loss: 0.2805, Train Acc: 0.8402 - Val Loss: 0.5791, Val Acc: 0.6134\n",
      "Fold 5, Epoch 19/25 - Train Loss: 0.2616, Train Acc: 0.8209 - Val Loss: 0.5584, Val Acc: 0.6134\n",
      "Fold 5, Epoch 20/25 - Train Loss: 0.2555, Train Acc: 0.8119 - Val Loss: 0.5619, Val Acc: 0.6134\n",
      "Fold 5, Epoch 21/25 - Train Loss: 0.2480, Train Acc: 0.8183 - Val Loss: 0.5960, Val Acc: 0.6031\n",
      "Fold 5, Epoch 22/25 - Train Loss: 0.2279, Train Acc: 0.8428 - Val Loss: 0.5822, Val Acc: 0.6082\n",
      "Fold 5, Epoch 23/25 - Train Loss: 0.2232, Train Acc: 0.8428 - Val Loss: 0.5549, Val Acc: 0.6392\n",
      "Fold 5, Epoch 24/25 - Train Loss: 0.2047, Train Acc: 0.8479 - Val Loss: 0.5845, Val Acc: 0.6237\n",
      "Fold 5, Epoch 25/25 - Train Loss: 0.2007, Train Acc: 0.8531 - Val Loss: 0.5811, Val Acc: 0.6340\n",
      "\n",
      "Classification Report for Fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      type-1       0.68      0.72      0.70        18\n",
      "      type-2       0.19      0.19      0.19        16\n",
      "      type-3       0.53      0.51      0.52        57\n",
      "      type-4       0.77      0.83      0.79        75\n",
      "      type-5       0.33      0.20      0.25         5\n",
      "      type-6       0.60      0.43      0.50         7\n",
      "      type-7       0.87      0.81      0.84        16\n",
      "\n",
      "    accuracy                           0.64       194\n",
      "   macro avg       0.57      0.53      0.54       194\n",
      "weighted avg       0.63      0.64      0.63       194\n",
      "\n",
      "\n",
      "--- Fold 5 Confusion Matrix ---\n",
      "[[13  2  2  1  0  0  0]\n",
      " [ 2  3  9  1  1  0  0]\n",
      " [ 1 10 29 14  0  1  2]\n",
      " [ 0  1 11 62  0  1  0]\n",
      " [ 2  0  1  1  1  0  0]\n",
      " [ 0  0  1  2  1  3  0]\n",
      " [ 1  0  2  0  0  0 13]]\n",
      "\n",
      "--- Fold 5 Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6842    0.7222    0.7027        18\n",
      "           1     0.1875    0.1875    0.1875        16\n",
      "           2     0.5273    0.5088    0.5179        57\n",
      "           3     0.7654    0.8267    0.7949        75\n",
      "           4     0.3333    0.2000    0.2500         5\n",
      "           5     0.6000    0.4286    0.5000         7\n",
      "           6     0.8667    0.8125    0.8387        16\n",
      "\n",
      "    accuracy                         0.6392       194\n",
      "   macro avg     0.5663    0.5266    0.5417       194\n",
      "weighted avg     0.6315    0.6392    0.6338       194\n",
      "\n",
      "['\\nFold Models:', ['Fold 1', 'Fold 2', 'Fold 3', 'Fold 4', 'Fold 5']]\n",
      "['Fold Accuracies:', [0.5721649484536082, 0.5773195876288659, 0.5567010309278351, 0.5773195876288659, 0.6391752577319587]]\n",
      "['Mean Accuracy:', np.float64(0.5845360824742267)]\n"
     ]
    }
   ],
   "source": [
    "kf_models, accs = run_kfold_training(\n",
    "    data_dir=DATA_DIR,\n",
    "    backbone='efficientnet_b3',  # or 'mobilenet_v3_small', 'efficientnet_b0', 'efficientnet_b3'\n",
    "    freeze_until_layer=None,  # or 'features.4' for EfficientNet\n",
    "    criterion_fn=FocalLoss(alpha=1, gamma=2),\n",
    "    num_epochs=25,\n",
    "    lr=1e-4,\n",
    "    k_folds=5,\n",
    "    batch_size=16,\n",
    "    train_transforms=train_transforms,\n",
    "    val_transforms=val_transforms,\n",
    "    seed=4,\n",
    "    num_classes=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b4979a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Single Split Training =======\n",
      "Class sample counts: [ 85 110 270 263  31  34  67]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold None, Epoch 1/15 - Train Loss: 1.3483, Train Acc: 0.2779 - Val Loss: 1.2287, Val Acc: 0.3116\n",
      "Fold None, Epoch 2/15 - Train Loss: 1.1277, Train Acc: 0.3791 - Val Loss: 0.9899, Val Acc: 0.4093\n",
      "Fold None, Epoch 3/15 - Train Loss: 0.9610, Train Acc: 0.4767 - Val Loss: 0.8523, Val Acc: 0.4512\n",
      "Fold None, Epoch 4/15 - Train Loss: 0.7984, Train Acc: 0.5744 - Val Loss: 0.7394, Val Acc: 0.5302\n",
      "Fold None, Epoch 5/15 - Train Loss: 0.6748, Train Acc: 0.6081 - Val Loss: 0.6649, Val Acc: 0.5488\n",
      "Fold None, Epoch 6/15 - Train Loss: 0.5889, Train Acc: 0.6500 - Val Loss: 0.6483, Val Acc: 0.5628\n",
      "Fold None, Epoch 7/15 - Train Loss: 0.5455, Train Acc: 0.6721 - Val Loss: 0.6470, Val Acc: 0.5395\n",
      "Fold None, Epoch 8/15 - Train Loss: 0.4737, Train Acc: 0.7012 - Val Loss: 0.6112, Val Acc: 0.5814\n",
      "Fold None, Epoch 9/15 - Train Loss: 0.4449, Train Acc: 0.7174 - Val Loss: 0.5528, Val Acc: 0.6279\n",
      "Fold None, Epoch 10/15 - Train Loss: 0.3791, Train Acc: 0.7337 - Val Loss: 0.5617, Val Acc: 0.6140\n",
      "Fold None, Epoch 11/15 - Train Loss: 0.3302, Train Acc: 0.7895 - Val Loss: 0.5432, Val Acc: 0.6093\n",
      "Fold None, Epoch 12/15 - Train Loss: 0.3167, Train Acc: 0.7640 - Val Loss: 0.5318, Val Acc: 0.6419\n",
      "Fold None, Epoch 13/15 - Train Loss: 0.2800, Train Acc: 0.7977 - Val Loss: 0.5164, Val Acc: 0.6558\n",
      "Fold None, Epoch 14/15 - Train Loss: 0.2485, Train Acc: 0.8233 - Val Loss: 0.5233, Val Acc: 0.6512\n",
      "Fold None, Epoch 15/15 - Train Loss: 0.2725, Train Acc: 0.8128 - Val Loss: 0.5209, Val Acc: 0.6465\n",
      "\n",
      "Classification Report for Fold None:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      type-1       0.60      0.86      0.71        21\n",
      "      type-2       0.59      0.57      0.58        28\n",
      "      type-3       0.62      0.60      0.61        68\n",
      "      type-4       0.70      0.65      0.67        65\n",
      "      type-5       0.67      0.50      0.57         8\n",
      "      type-6       0.71      0.62      0.67         8\n",
      "      type-7       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.66       215\n",
      "   macro avg       0.67      0.67      0.66       215\n",
      "weighted avg       0.66      0.66      0.65       215\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "[[18  1  0  0  0  0  2]\n",
      " [ 6 16  4  1  0  0  1]\n",
      " [ 3  6 41 17  0  0  1]\n",
      " [ 2  2 18 42  1  0  0]\n",
      " [ 0  0  2  0  4  2  0]\n",
      " [ 0  1  1  0  1  5  0]\n",
      " [ 1  1  0  0  0  0 15]]\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6000    0.8571    0.7059        21\n",
      "           1     0.5926    0.5714    0.5818        28\n",
      "           2     0.6212    0.6029    0.6119        68\n",
      "           3     0.7000    0.6462    0.6720        65\n",
      "           4     0.6667    0.5000    0.5714         8\n",
      "           5     0.7143    0.6250    0.6667         8\n",
      "           6     0.7895    0.8824    0.8333        17\n",
      "\n",
      "    accuracy                         0.6558       215\n",
      "   macro avg     0.6692    0.6693    0.6633       215\n",
      "weighted avg     0.6577    0.6558    0.6534       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model, best_acc = train_single_split(\n",
    "    data_dir=DATA_DIR,\n",
    "    model_name='mobilenet_v3_large',  # or 'mobilenet_v3_small', 'efficientnet_b0', 'efficientnet_b3'\n",
    "    freeze_until=None,  # or 'features.4', None for no freezing\n",
    "    criterion='focal',  # or 'smooth'\n",
    "    batch_size=16,\n",
    "    lr=1e-4,\n",
    "    num_epochs=15,\n",
    "    val_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f386f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Single Split Training =======\n",
      "Class sample counts: [100 111 270 262  31  34  67]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold None, Epoch 1/15 - Train Loss: 1.4252, Train Acc: 0.1497 - Val Loss: 1.3597, Val Acc: 0.2557\n",
      "Fold None, Epoch 2/15 - Train Loss: 1.3566, Train Acc: 0.2480 - Val Loss: 1.3260, Val Acc: 0.3059\n",
      "Fold None, Epoch 3/15 - Train Loss: 1.3074, Train Acc: 0.2926 - Val Loss: 1.2779, Val Acc: 0.3562\n",
      "Fold None, Epoch 4/15 - Train Loss: 1.2326, Train Acc: 0.3554 - Val Loss: 1.2260, Val Acc: 0.4018\n",
      "Fold None, Epoch 5/15 - Train Loss: 1.1748, Train Acc: 0.3851 - Val Loss: 1.1556, Val Acc: 0.4064\n",
      "Fold None, Epoch 6/15 - Train Loss: 1.1128, Train Acc: 0.4320 - Val Loss: 1.0714, Val Acc: 0.4521\n",
      "Fold None, Epoch 7/15 - Train Loss: 0.9950, Train Acc: 0.4983 - Val Loss: 0.9776, Val Acc: 0.4886\n",
      "Fold None, Epoch 8/15 - Train Loss: 0.9359, Train Acc: 0.5280 - Val Loss: 0.9076, Val Acc: 0.4977\n",
      "Fold None, Epoch 9/15 - Train Loss: 0.8530, Train Acc: 0.5543 - Val Loss: 0.8295, Val Acc: 0.5342\n",
      "Fold None, Epoch 10/15 - Train Loss: 0.7801, Train Acc: 0.5749 - Val Loss: 0.7839, Val Acc: 0.5297\n",
      "Fold None, Epoch 11/15 - Train Loss: 0.7227, Train Acc: 0.5989 - Val Loss: 0.7129, Val Acc: 0.5525\n",
      "Fold None, Epoch 12/15 - Train Loss: 0.6504, Train Acc: 0.6274 - Val Loss: 0.6431, Val Acc: 0.5799\n",
      "Fold None, Epoch 13/15 - Train Loss: 0.5668, Train Acc: 0.6663 - Val Loss: 0.6146, Val Acc: 0.5571\n",
      "Fold None, Epoch 14/15 - Train Loss: 0.5009, Train Acc: 0.6857 - Val Loss: 0.6075, Val Acc: 0.5525\n",
      "Fold None, Epoch 15/15 - Train Loss: 0.4435, Train Acc: 0.7257 - Val Loss: 0.5430, Val Acc: 0.6164\n",
      "\n",
      "Classification Report for Fold None:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      type-1       0.75      0.96      0.84        25\n",
      "      type-2       0.44      0.41      0.42        27\n",
      "      type-3       0.56      0.44      0.49        68\n",
      "      type-4       0.65      0.68      0.67        66\n",
      "      type-5       0.42      0.62      0.50         8\n",
      "      type-6       0.83      0.62      0.71         8\n",
      "      type-7       0.71      0.88      0.79        17\n",
      "\n",
      "    accuracy                           0.62       219\n",
      "   macro avg       0.62      0.66      0.63       219\n",
      "weighted avg       0.61      0.62      0.61       219\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "[[24  0  0  0  0  0  1]\n",
      " [ 5 11  7  2  1  0  1]\n",
      " [ 1 12 30 20  3  0  2]\n",
      " [ 2  1 17 45  0  0  1]\n",
      " [ 0  1  0  1  5  1  0]\n",
      " [ 0  0  0  1  1  5  1]\n",
      " [ 0  0  0  0  2  0 15]]\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7500    0.9600    0.8421        25\n",
      "           1     0.4400    0.4074    0.4231        27\n",
      "           2     0.5556    0.4412    0.4918        68\n",
      "           3     0.6522    0.6818    0.6667        66\n",
      "           4     0.4167    0.6250    0.5000         8\n",
      "           5     0.8333    0.6250    0.7143         8\n",
      "           6     0.7143    0.8824    0.7895        17\n",
      "\n",
      "    accuracy                         0.6164       219\n",
      "   macro avg     0.6231    0.6604    0.6325       219\n",
      "weighted avg     0.6100    0.6164    0.6076       219\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model, best_acc = train_single_split(\n",
    "    data_dir=DATA_DIR,\n",
    "    model_name='efficientnet_b3',\n",
    "    freeze_until=None,  # or 'features.4', None for no freezing. A higher layer means more layers are frozen.\n",
    "    criterion='focal', # 'focal' or 'smooth'\n",
    "    batch_size=32,\n",
    "    lr=1e-4,\n",
    "    num_epochs=20,\n",
    "    val_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869d1485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Single Split Training =======\n",
      "Class sample counts: [100 111 270 262  31  34  67]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sebastianapelgren/code/poop-ai/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold None, Epoch 1/20 - Train Loss: 1.4120, Train Acc: 0.1749 - Val Loss: 1.2051, Val Acc: 0.3425\n",
      "Fold None, Epoch 2/20 - Train Loss: 1.2750, Train Acc: 0.2720 - Val Loss: 1.1779, Val Acc: 0.3470\n",
      "Fold None, Epoch 3/20 - Train Loss: 1.1859, Train Acc: 0.3154 - Val Loss: 1.1232, Val Acc: 0.3790\n",
      "Fold None, Epoch 4/20 - Train Loss: 1.1256, Train Acc: 0.3749 - Val Loss: 1.0569, Val Acc: 0.3836\n",
      "Fold None, Epoch 5/20 - Train Loss: 1.0594, Train Acc: 0.4206 - Val Loss: 1.0140, Val Acc: 0.3927\n",
      "Fold None, Epoch 6/20 - Train Loss: 0.9753, Train Acc: 0.4720 - Val Loss: 0.9710, Val Acc: 0.4018\n",
      "Fold None, Epoch 7/20 - Train Loss: 0.9627, Train Acc: 0.4583 - Val Loss: 0.9287, Val Acc: 0.4521\n",
      "Fold None, Epoch 8/20 - Train Loss: 0.9476, Train Acc: 0.4514 - Val Loss: 0.8917, Val Acc: 0.4840\n",
      "Fold None, Epoch 9/20 - Train Loss: 0.8636, Train Acc: 0.5177 - Val Loss: 0.8702, Val Acc: 0.4703\n",
      "Fold None, Epoch 10/20 - Train Loss: 0.8627, Train Acc: 0.4949 - Val Loss: 0.8426, Val Acc: 0.4932\n",
      "Fold None, Epoch 11/20 - Train Loss: 0.8206, Train Acc: 0.5269 - Val Loss: 0.8383, Val Acc: 0.4977\n",
      "Fold None, Epoch 12/20 - Train Loss: 0.8445, Train Acc: 0.5029 - Val Loss: 0.8152, Val Acc: 0.4840\n",
      "Fold None, Epoch 13/20 - Train Loss: 0.8184, Train Acc: 0.5063 - Val Loss: 0.8145, Val Acc: 0.5023\n",
      "Fold None, Epoch 14/20 - Train Loss: 0.7660, Train Acc: 0.5394 - Val Loss: 0.7903, Val Acc: 0.5251\n",
      "Fold None, Epoch 15/20 - Train Loss: 0.7296, Train Acc: 0.5543 - Val Loss: 0.7722, Val Acc: 0.5251\n",
      "Fold None, Epoch 16/20 - Train Loss: 0.7258, Train Acc: 0.5463 - Val Loss: 0.8139, Val Acc: 0.5068\n",
      "Fold None, Epoch 17/20 - Train Loss: 0.6827, Train Acc: 0.5943 - Val Loss: 0.8204, Val Acc: 0.4658\n",
      "Fold None, Epoch 18/20 - Train Loss: 0.6815, Train Acc: 0.5920 - Val Loss: 0.7865, Val Acc: 0.4840\n",
      "Fold None, Epoch 19/20 - Train Loss: 0.6395, Train Acc: 0.6274 - Val Loss: 0.7706, Val Acc: 0.5160\n",
      "Fold None, Epoch 20/20 - Train Loss: 0.6568, Train Acc: 0.5966 - Val Loss: 0.7946, Val Acc: 0.5023\n",
      "\n",
      "Classification Report for Fold None:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      type-1       0.51      0.80      0.62        25\n",
      "      type-2       0.32      0.30      0.31        27\n",
      "      type-3       0.45      0.44      0.45        68\n",
      "      type-4       0.65      0.48      0.56        66\n",
      "      type-5       0.31      0.62      0.42         8\n",
      "      type-6       0.62      0.62      0.62         8\n",
      "      type-7       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.52       219\n",
      "   macro avg       0.53      0.58      0.54       219\n",
      "weighted avg       0.53      0.52      0.52       219\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "[[20  0  4  0  0  0  1]\n",
      " [ 5  8  7  2  2  2  1]\n",
      " [11  9 30 13  3  1  1]\n",
      " [ 3  4 23 32  4  0  0]\n",
      " [ 0  2  0  1  5  0  0]\n",
      " [ 0  1  0  1  1  5  0]\n",
      " [ 0  1  2  0  1  0 13]]\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5128    0.8000    0.6250        25\n",
      "           1     0.3200    0.2963    0.3077        27\n",
      "           2     0.4545    0.4412    0.4478        68\n",
      "           3     0.6531    0.4848    0.5565        66\n",
      "           4     0.3125    0.6250    0.4167         8\n",
      "           5     0.6250    0.6250    0.6250         8\n",
      "           6     0.8125    0.7647    0.7879        17\n",
      "\n",
      "    accuracy                         0.5160       219\n",
      "   macro avg     0.5272    0.5767    0.5381       219\n",
      "weighted avg     0.5333    0.5160    0.5152       219\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prestera inte jätte bra. mobilenet_v2 är bättre \n",
    "\n",
    "best_model, best_acc = train_single_split(\n",
    "    data_dir=DATA_DIR,\n",
    "    model_name='mobilenet_v3_large',\n",
    "    freeze_until='features.7',  # or 'features.4', None for no freezing.\n",
    "    criterion='focal', # 'focal' or 'smooth'\n",
    "    batch_size=32,\n",
    "    lr=1e-4,\n",
    "    num_epochs=20,\n",
    "    val_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c4e3a0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== ALL LOGS ====\n",
      "\n",
      "Classification Report for Fold None:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      type-1       0.72      0.72      0.72        25\n",
      "      type-2       0.55      0.72      0.62        25\n",
      "      type-3       0.53      0.49      0.51        65\n",
      "      type-4       0.72      0.64      0.68        61\n",
      "      type-5       0.50      0.83      0.62         6\n",
      "      type-6       0.60      0.60      0.60         5\n",
      "      type-7       0.80      0.80      0.80        15\n",
      "\n",
      "    accuracy                           0.63       202\n",
      "   macro avg       0.63      0.69      0.65       202\n",
      "weighted avg       0.64      0.63      0.63       202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs = get_logs()\n",
    "\n",
    "print(\"==== ALL LOGS ====\")\n",
    "for log in logs:\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b66e56a",
   "metadata": {},
   "source": [
    "# To load existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "366e5bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    model = models.efficientnet_b0()\n",
    "    in_features = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(in_features, 512),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(512, NUM_CLASSES)\n",
    "    )\n",
    "    state_dict = torch.load(model_path, map_location=DEVICE)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bd415c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../api/stool_model.pth\"\n",
    "\n",
    "\n",
    "# model = load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323635fe",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0d8adc",
   "metadata": {},
   "source": [
    "### Save as .pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dc010c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to stool_model.pth\n"
     ]
    }
   ],
   "source": [
    "# 1. Define where to save\n",
    "SAVE_PATH = \"../api/stool_model.pth\"\n",
    "\n",
    "# 2. Save the state_dict\n",
    "#torch.save(model.state_dict(), SAVE_PATH)\n",
    "print(f\"Model weights saved to {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bd490f",
   "metadata": {},
   "source": [
    "### Save as .onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cfcdfa02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ONNX export completed: stool_model.onnx\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Create dummy input for ONNX export (batch_size=1, 3 channels, IMG_SIZE x IMG_SIZE)\n",
    "dummy_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(\n",
    "    model,                               # your trained model\n",
    "    dummy_input,                         # input tensor\n",
    "    \"stool_model.onnx\",                  # output file name\n",
    "    export_params=True,                  # store weights inside the model file\n",
    "    opset_version=11,                    # ONNX opset version\n",
    "    do_constant_folding=True,            # fold constant values for optimization\n",
    "    input_names=['input'],               # name for the input layer\n",
    "    output_names=['output'],             # name for the output layer\n",
    "    dynamic_axes={                      # allow variable input sizes\n",
    "        'input': {0: 'batch_size'},     \n",
    "        'output': {0: 'batch_size'}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"✅ ONNX export completed: stool_model.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
